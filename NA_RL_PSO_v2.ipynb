{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s0han24/NA_RLPSO/blob/main/NA_RL_PSO_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zNiYfjRPYRcT",
        "outputId": "51f7c294-1408-4dff-86ad-287e3804266b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, scikit-optimize, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyaml-25.1.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision scikit-learn scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SlSwj7y2YRcV",
        "outputId": "1a160ba2-2b89-4ad2-ed65-12a1b24dae28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o1EJufEb42Mj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.datasets import load_digits, load_wine, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v8J-enbOYRcW"
      },
      "outputs": [],
      "source": [
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JRtB1CSO45AQ"
      },
      "outputs": [],
      "source": [
        "# Define objective function to minimize (Rastrigin function)\n",
        "def rastrigin(x):\n",
        "    A = 10\n",
        "    return A * len(x) + sum([(xi ** 2 - A * np.cos(2 * np.pi * xi)) for xi in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Rn_BWhFM5tDw"
      },
      "outputs": [],
      "source": [
        "# Particle Swarm Optimization implementation (baseline)\n",
        "class Particle:\n",
        "    def __init__(self, dim, bounds, objective=rastrigin, random_sequence=None):\n",
        "        # replace random with some sequence for experimentation\n",
        "        if random_sequence is not None:\n",
        "            self.position = random_sequence\n",
        "        else:\n",
        "            self.position = np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)])\n",
        "        self.velocity = np.zeros(dim)\n",
        "        self.best_position = np.copy(self.position)\n",
        "        self.best_value = objective(self.position)\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin, random_sequences=None):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.num_particles = num_particles\n",
        "        self.num_iter = num_iter\n",
        "        self.objective = objective\n",
        "        if random_sequences is None:\n",
        "            self.swarm = [Particle(dim, bounds, objective) for _ in range(num_particles)]\n",
        "        else :\n",
        "            self.swarm = [Particle(dim, bounds, objective, random_sequence) for random_sequence in random_sequences]\n",
        "        self.global_best_position = np.copy(self.swarm[0].position)\n",
        "        self.global_best_value = objective(self.global_best_position)\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "        c1, c2 = 1.5, 1.5\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "        for _ in range(self.num_iter):\n",
        "            X = []\n",
        "            V = []\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:, 0], self.bounds[:, 1])\n",
        "                value = self.objective(particle.position)\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SKfEtN_r4y_r"
      },
      "outputs": [],
      "source": [
        "# Policy Network for the RL component\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.action_mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Learnable log_std parameter for the Gaussian distribution\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mean = self.action_mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "    def select_action(self, state):\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum()  # Sum log probabilities across dimensions\n",
        "        # Clamp actions to a plausible range for c1 and c2 (e.g., [1.0, 2.0])\n",
        "        action = torch.clamp(action, 1.0, 2.0)\n",
        "        return action, log_prob\n",
        "\n",
        "# RL-enhanced PSO using the policy network to select acceleration coefficients c1 and c2\n",
        "class RLPSO(PSO):\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin, lr=1e-2, random_sequences=None):\n",
        "        super().__init__(dim, bounds, num_particles, num_iter, objective, random_sequences)\n",
        "        # Define a simple state: [iteration_ratio, global_best_value]\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 2  # one for c1 and one for c2\n",
        "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.objective = objective\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def get_state(self, iteration):\n",
        "        # Normalize iteration to [0, 1]\n",
        "        iter_norm = iteration / self.num_iter\n",
        "        # Use the current global best value (optionally, further normalization can be applied)\n",
        "        state = np.array([iter_norm, self.global_best_value], dtype=np.float32)\n",
        "        return torch.tensor(state)\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "\n",
        "        for it in tqdm(range(1, self.num_iter + 1), desc=\"Training progress\"):\n",
        "            state = self.get_state(it)\n",
        "            action, log_prob = self.policy_net.select_action(state)\n",
        "            c1, c2 = action.detach().numpy()  # use the selected coefficients for this iteration\n",
        "            X = []\n",
        "            V = []\n",
        "            # Store the old global best for reward computation\n",
        "            old_global_best = self.global_best_value\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:,0], self.bounds[:,1])\n",
        "                value = self.objective(particle.position)\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "\n",
        "            # Reward: improvement in global best value\n",
        "            reward = old_global_best - self.global_best_value\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            # Update policy network using the REINFORCE rule: maximize reward\n",
        "            loss = -log_prob * reward_tensor\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hcksn8R5xh8",
        "outputId": "18594064-fbee-426c-eea6-5978ad420735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running baseline PSO...\n",
            "\n",
            "Baseline PSO solution:\n",
            "Best position: [0.00031708 0.00110744]\n",
            "Best value: 0.0002632554577601809\n",
            "\n",
            "Running RL-enhanced PSO (RLPSO)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training progress: 100%|██████████| 30/30 [00:00<00:00, 65.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RLPSO solution:\n",
            "Best position: [ 7.26893824e-06 -8.93395422e-06]\n",
            "Best value: 2.6317309220758034e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Testing both the baseline PSO and the RL-enhanced PSO\n",
        "dim = 2\n",
        "bounds = np.array([[-2,2] for _ in range(dim)])\n",
        "num_particles = 40\n",
        "num_iter = 30\n",
        "\n",
        "# generate num_particles number of random sequences of length dim each\n",
        "random_sequences = [np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)]) for _ in range(num_particles)]\n",
        "\n",
        "\n",
        "print(\"Running baseline PSO...\")\n",
        "pso_solver = PSO(dim, bounds, num_particles, num_iter, random_sequences=random_sequences)\n",
        "best_position_pso, best_value_pso, X_list_pso, V_list_pso, pso_results = pso_solver.optimize()\n",
        "print(\"\\nBaseline PSO solution:\")\n",
        "print(\"Best position:\", best_position_pso)\n",
        "print(\"Best value:\", best_value_pso)\n",
        "\n",
        "print(\"\\nRunning RL-enhanced PSO (RLPSO)...\")\n",
        "rl_pso_solver = RLPSO(dim, bounds, num_particles, num_iter, random_sequences=random_sequences)\n",
        "best_position_rl, best_value_rl, X_list, V_list, rl_pso_results = rl_pso_solver.optimize()\n",
        "print(\"\\nRLPSO solution:\")\n",
        "print(\"Best position:\", best_position_rl)\n",
        "print(\"Best value:\", best_value_rl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1vtQOMmrcvVv",
        "outputId": "2e7be89e-a640-4420-99c8-0ab0831a90d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# prompt: plot the history from the pso algorithms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the history of the global best value for both PSO algorithms\n",
        "plt.plot(pso_results, label='PSO')\n",
        "plt.plot(rl_pso_results, label='RL-PSO')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Global Best Value')\n",
        "plt.title('Convergence History')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mcMtxh9X1Q_",
        "outputId": "7adc230c-088f-46db-d0ff-18cbe0f03ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-opt\n",
            "  Downloading scikit_opt-0.6.6-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scikit-opt) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scikit-opt) (1.15.2)\n",
            "Downloading scikit_opt-0.6.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: scikit-opt\n",
            "Successfully installed scikit-opt-0.6.6\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv0HrojN_qQ2"
      },
      "source": [
        "## Training some simple models to test hyperparameter tuning capabilities\n",
        "\n",
        "#### Model 1: Simple MLP for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "biKxefM8_pOO"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, hidden_size=128, dropout_rate=0.2):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tom0MLr4_zlN"
      },
      "outputs": [],
      "source": [
        "def train_mnist(\n",
        "    hidden_size=128,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    weight_decay=0.0,\n",
        "    momentum=0.9,\n",
        "    optimizer_type='adam'\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a simple MLP on MNIST dataset\n",
        "\n",
        "    Parameters:\n",
        "    - hidden_size: number of neurons in the hidden layer\n",
        "    - dropout_rate: dropout probability\n",
        "    - learning_rate: learning rate for the optimizer\n",
        "    - batch_size: size of mini-batches\n",
        "    - epochs: number of training epochs\n",
        "    - weight_decay: L2 regularization parameter\n",
        "    - momentum: momentum parameter (for SGD)\n",
        "    - optimizer_type: 'adam' or 'sgd'\n",
        "\n",
        "    Returns:\n",
        "    - trained model\n",
        "    - dictionary with training history\n",
        "    \"\"\"\n",
        "    if batch_size < 32:\n",
        "        batch_size = 2**batch_size\n",
        "\n",
        "\n",
        "    # Data loading\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleMLP(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = correct / total\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "082TtNIzAG44"
      },
      "source": [
        "#### Model 2: CNN for CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cZh5ExCN_7yM"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, filters1=32, filters2=64, dropout_rate=0.25):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, filters1, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(filters1)\n",
        "        self.conv2 = nn.Conv2d(filters1, filters2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(filters2)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(filters2 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lwo5_IKfAVC6"
      },
      "outputs": [],
      "source": [
        "def train_cifar10(\n",
        "    filters1=32,\n",
        "    filters2=64,\n",
        "    dropout_rate=0.25,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    epochs=10,\n",
        "    weight_decay=0.0001,\n",
        "    optimizer_type='adam'\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a simple CNN on CIFAR-10 dataset\n",
        "\n",
        "    Parameters:\n",
        "    - filters1: number of filters in first conv layer\n",
        "    - filters2: number of filters in second conv layer\n",
        "    - dropout_rate: dropout probability\n",
        "    - learning_rate: learning rate for the optimizer\n",
        "    - batch_size: size of mini-batches\n",
        "    - epochs: number of training epochs\n",
        "    - weight_decay: L2 regularization parameter\n",
        "    - optimizer_type: 'adam' or 'sgd'\n",
        "\n",
        "    Returns:\n",
        "    - trained model\n",
        "    - dictionary with training history\n",
        "    \"\"\"\n",
        "    if batch_size < 32:\n",
        "        batch_size = 2**batch_size\n",
        "\n",
        "    # Data loading\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleCNN(filters1=filters1, filters2=filters2, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "\n",
        "    # Training loop\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = correct / total\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(test_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQFo-7gAl3s"
      },
      "source": [
        "## Baseline methods for hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b0Ors2tzAahr"
      },
      "outputs": [],
      "source": [
        "def random_search(model_func, param_grid, n_trials=10):\n",
        "    \"\"\"\n",
        "    Perform random search for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible values\n",
        "    - n_trials: number of random combinations to try\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "\n",
        "    for _ in tqdm(range(n_trials)):\n",
        "        # Sample random hyperparameters\n",
        "        params = {k: random.choice(v) if isinstance(v, list) else v for k, v in param_grid.items()}\n",
        "        params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "\n",
        "        # Train model with sampled hyperparameters\n",
        "        _, history = model_func(**params)\n",
        "\n",
        "        # Use the best test accuracy as the score\n",
        "        score = max(history['test_acc'])\n",
        "\n",
        "        results.append((params, score))\n",
        "\n",
        "        # Update best parameters if needed\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "T2mTAKBbAytC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def grid_search(model_func, param_grid):\n",
        "    \"\"\"\n",
        "    Perform grid search for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible values\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate all combinations of parameters\n",
        "    keys = param_grid.keys()\n",
        "\n",
        "    # Make sure all values are iterable (convert single values to lists)\n",
        "    values = []\n",
        "    for key in keys:\n",
        "        if isinstance(param_grid[key], (list, tuple)):\n",
        "            values.append(param_grid[key])\n",
        "        else:\n",
        "            # If the parameter is a single value (like an integer), wrap it in a list\n",
        "            values.append([param_grid[key]])\n",
        "\n",
        "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    results = []\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "\n",
        "    for params in tqdm(param_combinations):\n",
        "        # Train model with parameters\n",
        "        _, history = model_func(**params)\n",
        "\n",
        "        # Use the best test accuracy as the score\n",
        "        score = max(history['test_acc'])\n",
        "\n",
        "        results.append((params, score))\n",
        "\n",
        "        # Update best parameters if needed\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score, results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Particle Swarm Optimization implementation (baseline)\n",
        "class Particle:\n",
        "    def __init__(self, dim, bounds, random_sequence=None):\n",
        "        # replace random with some sequence for experimentation\n",
        "        if random_sequence is not None:\n",
        "            self.position = random_sequence\n",
        "        else:\n",
        "            self.position = np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)])\n",
        "        self.velocity = np.zeros(dim)\n",
        "        self.best_position = np.copy(self.position)\n",
        "        # use train_mnist as the objective function\n",
        "        params = {k: v for k, v in zip(param_grid_mnist.keys(), self.position)}\n",
        "        params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "        for k in ['batch_size', 'hidden_size', 'epochs']:\n",
        "            params[k] = int(params[k])\n",
        "        self.best_value = train_mnist(**params)[1]['test_acc'][-1]\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, random_sequences=None):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.num_particles = num_particles\n",
        "        self.num_iter = num_iter\n",
        "        if random_sequences is None:\n",
        "            self.swarm = [Particle(dim, bounds) for _ in range(num_particles)]\n",
        "        else :\n",
        "            self.swarm = [Particle(dim, bounds, random_sequence) for random_sequence in random_sequences]\n",
        "        self.global_best_position = np.copy(self.swarm[0].position)\n",
        "        self.global_best_value = self.swarm[0].best_value\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "        c1, c2 = 1.5, 1.5\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "        for _ in range(self.num_iter):\n",
        "            X = []\n",
        "            V = []\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:, 0], self.bounds[:, 1])\n",
        "                params = {k: v for k, v in zip(param_grid_mnist.keys(), particle.position)}\n",
        "                params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "                params['batch_size'] = int(params['batch_size'])\n",
        "                for k in ['batch_size', 'hidden_size', 'epochs']:\n",
        "                    params[k] = int(params[k])\n",
        "                value = train_mnist(**params)[1]['test_acc'][-1]\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "\n",
        "            print('best value: ', self.global_best_value)\n",
        "\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results"
      ],
      "metadata": {
        "id": "GAYyHRxpqowN"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UA-hwhSjCOSt"
      },
      "outputs": [],
      "source": [
        "def PSO_hyperparam(model_func, param_grid, num_particles=10, num_iter=5, w=0.7, c1=2, c2=2):\n",
        "    \"\"\"\n",
        "    Perform PSO for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible\n",
        "    values\n",
        "    - num_particles: number of particles in the PSO\n",
        "    - num_iter: number of iterations\n",
        "    - w: inertia weight\n",
        "    - c1: cognitive\n",
        "    - c2: social\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "\n",
        "    pso_solver = PSO(dim=len(param_grid), bounds=np.array([[min(v), max(v)] for v in param_grid.values()]), num_particles=num_particles, num_iter=num_iter)\n",
        "    best_position, best_value, X_list, V_list, pso_results = pso_solver.optimize()\n",
        "    best_params = {k: v for k, v in zip(param_grid.keys(), best_position)}\n",
        "    return best_params, best_value, pso_results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy Network for the RL component\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.action_mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Learnable log_std parameter for the Gaussian distribution\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mean = self.action_mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "    def select_action(self, state):\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum()  # Sum log probabilities across dimensions\n",
        "        # Clamp actions to a plausible range for c1 and c2 (e.g., [1.0, 2.0])\n",
        "        action = torch.clamp(action, 1.0, 2.0)\n",
        "        return action, log_prob\n",
        "\n",
        "# RL-enhanced PSO using the policy network to select acceleration coefficients c1 and c2\n",
        "class RLPSO(PSO):\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, lr=1e-2, random_sequences=None):\n",
        "        super().__init__(dim, bounds, num_particles, num_iter, objective, random_sequences)\n",
        "        # Define a simple state: [iteration_ratio, global_best_value]\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 2  # one for c1 and one for c2\n",
        "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def get_state(self, iteration):\n",
        "        # Normalize iteration to [0, 1]\n",
        "        iter_norm = iteration / self.num_iter\n",
        "        # Use the current global best value (optionally, further normalization can be applied)\n",
        "        state = np.array([iter_norm, self.global_best_value], dtype=np.float32)\n",
        "        return torch.tensor(state)\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "\n",
        "        for it in tqdm(range(1, self.num_iter + 1), desc=\"Training progress\"):\n",
        "            state = self.get_state(it)\n",
        "            action, log_prob = self.policy_net.select_action(state)\n",
        "            c1, c2 = action.detach().numpy()  # use the selected coefficients for this iteration\n",
        "            X = []\n",
        "            V = []\n",
        "            # Store the old global best for reward computation\n",
        "            old_global_best = self.global_best_value\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:,0], self.bounds[:,1])\n",
        "                # use train_mnist as the objective function\n",
        "                params = {k: v for k, v in zip(param_grid_mnist.keys(), particle.position)}\n",
        "                params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "                for k in ['batch_size', 'hidden_size', 'epochs']:\n",
        "                    params[k] = int(params[k])\n",
        "                value = train_mnist(**params)[1]['test_acc'][-1]\n",
        "\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "\n",
        "            # Reward: improvement in global best value\n",
        "            reward = old_global_best - self.global_best_value\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            # Update policy network using the REINFORCE rule: maximize reward\n",
        "            loss = -log_prob * reward_tensor\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results\n"
      ],
      "metadata": {
        "id": "61QAqUyByPQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AT9IWIuKCAdl"
      },
      "outputs": [],
      "source": [
        "def RL_PSO_hyperparam(model_func, param_grid, num_particles=10, num_iter=5):\n",
        "    rl_pso_solver = RLPSO(dim=len(param_grid), bounds=np.array([[min(v), max(v)] for v in param_grid.values()]), num_particles=num_particles, num_iter=num_iter)\n",
        "    best_position, best_value, X_list, V_list, rl_pso_results = rl_pso_solver.optimize()\n",
        "    best_params = {k: v for k, v in zip(param_grid.keys(), best_position)}\n",
        "    return best_params, best_value, rl_pso_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AZr6QBaxYRcj"
      },
      "outputs": [],
      "source": [
        "from skopt import gp_minimize\n",
        "\n",
        "def bayesian_optimization(model_func, param_space, n_calls=10):\n",
        "    \"\"\"\n",
        "    Perform Bayesian Optimization for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_space: list of skopt.space dimensions defining the search space\n",
        "    - n_calls: number of iterations for optimization\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the objective function (minimize negative accuracy)\n",
        "    def objective(params):\n",
        "        param_dict = {dim.name: param for dim, param in zip(param_space, params)}\n",
        "        param_dict['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "        param_dict['batch_size'] = int(param_dict['batch_size'])\n",
        "        _, history = model_func(**param_dict)\n",
        "        return -max(history['test_acc'])  # Return negative accuracy for minimization\n",
        "\n",
        "    # Run Bayesian optimization\n",
        "    result = gp_minimize(\n",
        "        objective,\n",
        "        param_space,\n",
        "        n_calls=n_calls,\n",
        "        random_state=42,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Extract the best parameters\n",
        "    best_params = {dim.name: result.x[i] for i, dim in enumerate(param_space)}\n",
        "    best_score = -result.fun\n",
        "\n",
        "    return best_params, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Qhp9zi5ZA5q2"
      },
      "outputs": [],
      "source": [
        "param_grid_mnist = {\n",
        "    'hidden_size': [64, 128, 256],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'batch_size': [5, 6, 7],\n",
        "    'epochs': [3,4],  # Fixed for faster execution\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yNPWvBA-A9EB",
        "outputId": "f28a65f4-8b1f-4a76-f3c9-bd212e05af34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example 1: MNIST\n",
            "\n",
            "Random Search\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "  0%|          | 0.00/9.91M [00:00<?, ?B/s]\u001b[A\n",
            "  1%|          | 65.5k/9.91M [00:00<00:18, 544kB/s]\u001b[A\n",
            "  3%|▎         | 295k/9.91M [00:00<00:07, 1.33MB/s]\u001b[A\n",
            " 13%|█▎        | 1.25M/9.91M [00:00<00:02, 4.26MB/s]\u001b[A\n",
            " 51%|█████     | 5.05M/9.91M [00:00<00:00, 14.8MB/s]\u001b[A\n",
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n",
            "\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 496kB/s]\n",
            "\n",
            "  0%|          | 0.00/1.65M [00:00<?, ?B/s]\u001b[A\n",
            "  4%|▍         | 65.5k/1.65M [00:00<00:02, 546kB/s]\u001b[A\n",
            " 10%|▉         | 164k/1.65M [00:00<00:02, 697kB/s] \u001b[A\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.83MB/s]\n",
            "\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.31MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Train Loss: 0.2767, Test Loss: 0.1348, Test Acc: 0.9574\n",
            "Epoch 2/3, Train Loss: 0.1439, Test Loss: 0.1110, Test Acc: 0.9681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 1/5 [00:55<03:43, 55.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1158, Test Loss: 0.0984, Test Acc: 0.9721\n",
            "Epoch 1/3, Train Loss: 0.2951, Test Loss: 0.1368, Test Acc: 0.9570\n",
            "Epoch 2/3, Train Loss: 0.1645, Test Loss: 0.1019, Test Acc: 0.9692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 2/5 [01:47<02:40, 53.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1358, Test Loss: 0.0926, Test Acc: 0.9709\n",
            "Epoch 1/3, Train Loss: 0.5290, Test Loss: 0.2677, Test Acc: 0.9245\n",
            "Epoch 2/3, Train Loss: 0.2720, Test Loss: 0.2050, Test Acc: 0.9420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 3/5 [02:38<01:44, 52.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2166, Test Loss: 0.1682, Test Acc: 0.9515\n",
            "Epoch 1/3, Train Loss: 0.6727, Test Loss: 0.3161, Test Acc: 0.9149\n",
            "Epoch 2/3, Train Loss: 0.3327, Test Loss: 0.2460, Test Acc: 0.9294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 4/5 [03:22<00:49, 49.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2725, Test Loss: 0.2084, Test Acc: 0.9399\n",
            "Epoch 1/3, Train Loss: 0.6352, Test Loss: 0.3147, Test Acc: 0.9147\n",
            "Epoch 2/3, Train Loss: 0.3211, Test Loss: 0.2502, Test Acc: 0.9280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [04:11<00:00, 50.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2630, Test Loss: 0.2150, Test Acc: 0.9368\n",
            "Best accuracy: 0.9721\n",
            "Best parameters: {'hidden_size': 64, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 3, 'optimizer_type': 'adam'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nExample 1: MNIST\")\n",
        "print(\"\\nRandom Search\")\n",
        "best_params, best_score, results = random_search(train_mnist, param_grid_mnist, n_trials=5)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FVn3UHHmBE4F",
        "outputId": "bb28e1d4-52c3-4038-b066-4835105eb5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Grid Search\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/81 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Train Loss: 0.5999, Test Loss: 0.3087, Test Acc: 0.9169\n",
            "Epoch 2/3, Train Loss: 0.3105, Test Loss: 0.2491, Test Acc: 0.9296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 1/81 [00:50<1:07:04, 50.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2574, Test Loss: 0.2095, Test Acc: 0.9396\n",
            "Epoch 1/3, Train Loss: 0.7384, Test Loss: 0.3496, Test Acc: 0.9070\n",
            "Epoch 2/3, Train Loss: 0.3526, Test Loss: 0.2781, Test Acc: 0.9192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 2/81 [01:34<1:01:33, 46.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2934, Test Loss: 0.2399, Test Acc: 0.9306\n",
            "Epoch 1/3, Train Loss: 0.9114, Test Loss: 0.4315, Test Acc: 0.8924\n",
            "Epoch 2/3, Train Loss: 0.4152, Test Loss: 0.3234, Test Acc: 0.9101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▎         | 3/81 [02:16<57:54, 44.55s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3407, Test Loss: 0.2811, Test Acc: 0.9208\n",
            "Epoch 1/3, Train Loss: 0.2884, Test Loss: 0.1539, Test Acc: 0.9526\n",
            "Epoch 2/3, Train Loss: 0.1498, Test Loss: 0.1198, Test Acc: 0.9652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▍         | 4/81 [03:06<59:43, 46.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1205, Test Loss: 0.0931, Test Acc: 0.9709\n",
            "Epoch 1/3, Train Loss: 0.3228, Test Loss: 0.1583, Test Acc: 0.9525\n",
            "Epoch 2/3, Train Loss: 0.1649, Test Loss: 0.1210, Test Acc: 0.9634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 5/81 [03:50<58:08, 45.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1276, Test Loss: 0.1059, Test Acc: 0.9678\n",
            "Epoch 1/3, Train Loss: 0.3619, Test Loss: 0.1919, Test Acc: 0.9448\n",
            "Epoch 2/3, Train Loss: 0.1862, Test Loss: 0.1352, Test Acc: 0.9602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 6/81 [04:32<55:42, 44.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1432, Test Loss: 0.1120, Test Acc: 0.9663\n",
            "Epoch 1/3, Train Loss: 0.4012, Test Loss: 0.2482, Test Acc: 0.9265\n",
            "Epoch 2/3, Train Loss: 0.3502, Test Loss: 0.2742, Test Acc: 0.9309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▊         | 7/81 [05:23<57:31, 46.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3423, Test Loss: 0.3135, Test Acc: 0.9268\n",
            "Epoch 1/3, Train Loss: 0.3512, Test Loss: 0.2323, Test Acc: 0.9379\n",
            "Epoch 2/3, Train Loss: 0.2693, Test Loss: 0.2088, Test Acc: 0.9421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 8/81 [06:10<56:38, 46.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2580, Test Loss: 0.2035, Test Acc: 0.9425\n",
            "Epoch 1/3, Train Loss: 0.3090, Test Loss: 0.1794, Test Acc: 0.9428\n",
            "Epoch 2/3, Train Loss: 0.2210, Test Loss: 0.1671, Test Acc: 0.9482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 9/81 [07:09<1:00:46, 50.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1996, Test Loss: 0.1528, Test Acc: 0.9594\n",
            "Epoch 1/3, Train Loss: 0.6413, Test Loss: 0.3140, Test Acc: 0.9112\n",
            "Epoch 2/3, Train Loss: 0.3397, Test Loss: 0.2533, Test Acc: 0.9258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 10/81 [08:11<1:04:00, 54.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2826, Test Loss: 0.2199, Test Acc: 0.9360\n",
            "Epoch 1/3, Train Loss: 0.7821, Test Loss: 0.3631, Test Acc: 0.9053\n",
            "Epoch 2/3, Train Loss: 0.3808, Test Loss: 0.2807, Test Acc: 0.9214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▎        | 11/81 [08:58<1:00:26, 51.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3174, Test Loss: 0.2423, Test Acc: 0.9308\n",
            "Epoch 1/3, Train Loss: 0.9901, Test Loss: 0.4513, Test Acc: 0.8905\n",
            "Epoch 2/3, Train Loss: 0.4511, Test Loss: 0.3296, Test Acc: 0.9111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▍        | 12/81 [09:40<56:25, 49.06s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3645, Test Loss: 0.2826, Test Acc: 0.9185\n",
            "Epoch 1/3, Train Loss: 0.3108, Test Loss: 0.1468, Test Acc: 0.9550\n",
            "Epoch 2/3, Train Loss: 0.1737, Test Loss: 0.1152, Test Acc: 0.9654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 13/81 [10:31<56:11, 49.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1453, Test Loss: 0.1024, Test Acc: 0.9712\n",
            "Epoch 1/3, Train Loss: 0.3560, Test Loss: 0.1781, Test Acc: 0.9472\n",
            "Epoch 2/3, Train Loss: 0.1933, Test Loss: 0.1272, Test Acc: 0.9617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 14/81 [11:18<54:20, 48.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1582, Test Loss: 0.1091, Test Acc: 0.9664\n",
            "Epoch 1/3, Train Loss: 0.4196, Test Loss: 0.2036, Test Acc: 0.9406\n",
            "Epoch 2/3, Train Loss: 0.2172, Test Loss: 0.1453, Test Acc: 0.9568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▊        | 15/81 [12:02<52:09, 47.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1709, Test Loss: 0.1216, Test Acc: 0.9645\n",
            "Epoch 1/3, Train Loss: 0.4835, Test Loss: 0.2963, Test Acc: 0.9223\n",
            "Epoch 2/3, Train Loss: 0.4290, Test Loss: 0.2957, Test Acc: 0.9213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|█▉        | 16/81 [12:54<52:41, 48.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.4177, Test Loss: 0.3025, Test Acc: 0.9188\n",
            "Epoch 1/3, Train Loss: 0.4089, Test Loss: 0.2051, Test Acc: 0.9390\n",
            "Epoch 2/3, Train Loss: 0.3322, Test Loss: 0.1987, Test Acc: 0.9439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 17/81 [13:40<51:01, 47.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3099, Test Loss: 0.2024, Test Acc: 0.9477\n",
            "Epoch 1/3, Train Loss: 0.3766, Test Loss: 0.1923, Test Acc: 0.9429\n",
            "Epoch 2/3, Train Loss: 0.2774, Test Loss: 0.1967, Test Acc: 0.9419\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 18/81 [14:22<48:33, 46.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2500, Test Loss: 0.1723, Test Acc: 0.9493\n",
            "Epoch 1/3, Train Loss: 0.6881, Test Loss: 0.3196, Test Acc: 0.9117\n",
            "Epoch 2/3, Train Loss: 0.3620, Test Loss: 0.2524, Test Acc: 0.9259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 19/81 [15:13<49:02, 47.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3034, Test Loss: 0.2179, Test Acc: 0.9360\n",
            "Epoch 1/3, Train Loss: 0.8480, Test Loss: 0.3769, Test Acc: 0.9008\n",
            "Epoch 2/3, Train Loss: 0.4206, Test Loss: 0.2922, Test Acc: 0.9190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▍       | 20/81 [15:58<47:35, 46.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3509, Test Loss: 0.2500, Test Acc: 0.9278\n",
            "Epoch 1/3, Train Loss: 1.0201, Test Loss: 0.4615, Test Acc: 0.8861\n",
            "Epoch 2/3, Train Loss: 0.4928, Test Loss: 0.3407, Test Acc: 0.9068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 21/81 [16:41<45:43, 45.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.4020, Test Loss: 0.2946, Test Acc: 0.9162\n",
            "Epoch 1/3, Train Loss: 0.3642, Test Loss: 0.1620, Test Acc: 0.9484\n",
            "Epoch 2/3, Train Loss: 0.2229, Test Loss: 0.1247, Test Acc: 0.9620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 22/81 [17:31<46:18, 47.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1949, Test Loss: 0.1160, Test Acc: 0.9639\n",
            "Epoch 1/3, Train Loss: 0.3842, Test Loss: 0.1743, Test Acc: 0.9478\n",
            "Epoch 2/3, Train Loss: 0.2208, Test Loss: 0.1310, Test Acc: 0.9585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 23/81 [18:16<44:56, 46.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1833, Test Loss: 0.1139, Test Acc: 0.9665\n",
            "Epoch 1/3, Train Loss: 0.4493, Test Loss: 0.2031, Test Acc: 0.9390\n",
            "Epoch 2/3, Train Loss: 0.2467, Test Loss: 0.1470, Test Acc: 0.9558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|██▉       | 24/81 [18:58<42:52, 45.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2077, Test Loss: 0.1307, Test Acc: 0.9609\n",
            "Epoch 1/3, Train Loss: 0.5846, Test Loss: 0.2707, Test Acc: 0.9236\n",
            "Epoch 2/3, Train Loss: 0.5169, Test Loss: 0.3371, Test Acc: 0.9125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 25/81 [19:49<43:37, 46.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.5096, Test Loss: 0.2830, Test Acc: 0.9267\n",
            "Epoch 1/3, Train Loss: 0.4794, Test Loss: 0.2190, Test Acc: 0.9386\n",
            "Epoch 2/3, Train Loss: 0.4035, Test Loss: 0.2334, Test Acc: 0.9407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 26/81 [20:34<42:24, 46.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3791, Test Loss: 0.2273, Test Acc: 0.9406\n",
            "Epoch 1/3, Train Loss: 0.4590, Test Loss: 0.2198, Test Acc: 0.9325\n",
            "Epoch 2/3, Train Loss: 0.3491, Test Loss: 0.2021, Test Acc: 0.9372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 27/81 [21:16<40:33, 45.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3277, Test Loss: 0.1854, Test Acc: 0.9467\n",
            "Epoch 1/3, Train Loss: 0.5022, Test Loss: 0.2727, Test Acc: 0.9238\n",
            "Epoch 2/3, Train Loss: 0.2597, Test Loss: 0.2077, Test Acc: 0.9399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 28/81 [22:08<41:31, 47.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2027, Test Loss: 0.1677, Test Acc: 0.9507\n",
            "Epoch 1/3, Train Loss: 0.6256, Test Loss: 0.3125, Test Acc: 0.9138\n",
            "Epoch 2/3, Train Loss: 0.3024, Test Loss: 0.2448, Test Acc: 0.9293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 29/81 [22:53<40:07, 46.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2477, Test Loss: 0.2064, Test Acc: 0.9396\n",
            "Epoch 1/3, Train Loss: 0.7902, Test Loss: 0.3674, Test Acc: 0.9021\n",
            "Epoch 2/3, Train Loss: 0.3490, Test Loss: 0.2803, Test Acc: 0.9208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 30/81 [23:35<38:28, 45.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2847, Test Loss: 0.2396, Test Acc: 0.9318\n",
            "Epoch 1/3, Train Loss: 0.2467, Test Loss: 0.1299, Test Acc: 0.9596\n",
            "Epoch 2/3, Train Loss: 0.1195, Test Loss: 0.0921, Test Acc: 0.9710\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 31/81 [24:26<39:00, 46.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.0905, Test Loss: 0.0970, Test Acc: 0.9715\n",
            "Epoch 1/3, Train Loss: 0.2767, Test Loss: 0.1337, Test Acc: 0.9595\n",
            "Epoch 2/3, Train Loss: 0.1280, Test Loss: 0.1024, Test Acc: 0.9699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 32/81 [25:11<37:47, 46.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.0951, Test Loss: 0.0909, Test Acc: 0.9715\n",
            "Epoch 1/3, Train Loss: 0.3210, Test Loss: 0.1629, Test Acc: 0.9522\n",
            "Epoch 2/3, Train Loss: 0.1472, Test Loss: 0.1110, Test Acc: 0.9670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 41%|████      | 33/81 [25:53<36:05, 45.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1077, Test Loss: 0.0963, Test Acc: 0.9701\n",
            "Epoch 1/3, Train Loss: 0.4019, Test Loss: 0.2444, Test Acc: 0.9307\n",
            "Epoch 2/3, Train Loss: 0.3479, Test Loss: 0.3207, Test Acc: 0.9198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 34/81 [26:45<37:00, 47.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.3347, Test Loss: 0.3054, Test Acc: 0.9342\n",
            "Epoch 1/3, Train Loss: 0.3376, Test Loss: 0.1990, Test Acc: 0.9434\n",
            "Epoch 2/3, Train Loss: 0.2529, Test Loss: 0.2313, Test Acc: 0.9368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 35/81 [27:30<35:32, 46.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2500, Test Loss: 0.2214, Test Acc: 0.9498\n",
            "Epoch 1/3, Train Loss: 0.2905, Test Loss: 0.1935, Test Acc: 0.9376\n",
            "Epoch 2/3, Train Loss: 0.1926, Test Loss: 0.1928, Test Acc: 0.9463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 36/81 [28:12<33:51, 45.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.1787, Test Loss: 0.1861, Test Acc: 0.9554\n",
            "Epoch 1/3, Train Loss: 0.5266, Test Loss: 0.2755, Test Acc: 0.9230\n",
            "Epoch 2/3, Train Loss: 0.2766, Test Loss: 0.2146, Test Acc: 0.9377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 37/81 [29:02<34:11, 46.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2196, Test Loss: 0.1719, Test Acc: 0.9510\n",
            "Epoch 1/3, Train Loss: 0.6473, Test Loss: 0.3106, Test Acc: 0.9159\n",
            "Epoch 2/3, Train Loss: 0.3121, Test Loss: 0.2454, Test Acc: 0.9287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 38/81 [29:47<33:06, 46.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2561, Test Loss: 0.2066, Test Acc: 0.9382\n",
            "Epoch 1/3, Train Loss: 0.7911, Test Loss: 0.3675, Test Acc: 0.9032\n",
            "Epoch 2/3, Train Loss: 0.3628, Test Loss: 0.2821, Test Acc: 0.9199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 39/81 [30:29<31:20, 44.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Train Loss: 0.2974, Test Loss: 0.2413, Test Acc: 0.9314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 39/81 [30:42<33:03, 47.24s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2331fbae6175>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Grid Search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best accuracy: {best_score:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters: {best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c84f0cb8dfc6>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(model_func, param_grid)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_combinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Train model with parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Use the best test accuracy as the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-27e88aa407e2>\u001b[0m in \u001b[0;36mtrain_mnist\u001b[0;34m(hidden_size, dropout_rate, learning_rate, batch_size, epochs, weight_decay, momentum, optimizer_type)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"std evaluated to zero after conversion to {dtype}, leading to division by zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"\\n Grid Search\")\n",
        "best_params, best_score, results = grid_search(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_grTOLQIBIBV",
        "outputId": "ba41a40f-6701-410b-c686-3b7e44939412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bayesian Optimization\n",
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.2338, Test Loss: 0.1300, Test Acc: 0.9604\n",
            "Epoch 2/3, Train Loss: 0.1470, Test Loss: 0.1264, Test Acc: 0.9641\n",
            "Epoch 3/3, Train Loss: 0.1261, Test Loss: 0.1035, Test Acc: 0.9700\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 49.3465\n",
            "Function value obtained: -0.9700\n",
            "Current minimum: -0.9700\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.3520, Test Loss: 0.1728, Test Acc: 0.9473\n",
            "Epoch 2/4, Train Loss: 0.1849, Test Loss: 0.1276, Test Acc: 0.9613\n",
            "Epoch 3/4, Train Loss: 0.1455, Test Loss: 0.1049, Test Acc: 0.9668\n",
            "Epoch 4/4, Train Loss: 0.1234, Test Loss: 0.0963, Test Acc: 0.9709\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 71.4554\n",
            "Function value obtained: -0.9709\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.4466, Test Loss: 0.2436, Test Acc: 0.9297\n",
            "Epoch 2/4, Train Loss: 0.3824, Test Loss: 0.2134, Test Acc: 0.9415\n",
            "Epoch 3/4, Train Loss: 0.3718, Test Loss: 0.2503, Test Acc: 0.9337\n",
            "Epoch 4/4, Train Loss: 0.3576, Test Loss: 0.2506, Test Acc: 0.9402\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 71.6607\n",
            "Function value obtained: -0.9415\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.4831, Test Loss: 0.2562, Test Acc: 0.9262\n",
            "Epoch 2/4, Train Loss: 0.2508, Test Loss: 0.1937, Test Acc: 0.9436\n",
            "Epoch 3/4, Train Loss: 0.1937, Test Loss: 0.1569, Test Acc: 0.9544\n",
            "Epoch 4/4, Train Loss: 0.1593, Test Loss: 0.1302, Test Acc: 0.9619\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 71.1054\n",
            "Function value obtained: -0.9619\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.3840, Test Loss: 0.2470, Test Acc: 0.9272\n",
            "Epoch 2/3, Train Loss: 0.3253, Test Loss: 0.2400, Test Acc: 0.9398\n",
            "Epoch 3/3, Train Loss: 0.3111, Test Loss: 0.2525, Test Acc: 0.9391\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 53.1378\n",
            "Function value obtained: -0.9398\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.3511, Test Loss: 0.2108, Test Acc: 0.9378\n",
            "Epoch 2/4, Train Loss: 0.2875, Test Loss: 0.2749, Test Acc: 0.9183\n",
            "Epoch 3/4, Train Loss: 0.2727, Test Loss: 0.2064, Test Acc: 0.9489\n",
            "Epoch 4/4, Train Loss: 0.2545, Test Loss: 0.2224, Test Acc: 0.9538\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 63.8745\n",
            "Function value obtained: -0.9538\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.6991, Test Loss: 0.3319, Test Acc: 0.9076\n",
            "Epoch 2/4, Train Loss: 0.3268, Test Loss: 0.2591, Test Acc: 0.9271\n",
            "Epoch 3/4, Train Loss: 0.2657, Test Loss: 0.2205, Test Acc: 0.9365\n",
            "Epoch 4/4, Train Loss: 0.2275, Test Loss: 0.1896, Test Acc: 0.9442\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 58.2896\n",
            "Function value obtained: -0.9442\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.3387, Test Loss: 0.1826, Test Acc: 0.9461\n",
            "Epoch 2/4, Train Loss: 0.1658, Test Loss: 0.1241, Test Acc: 0.9639\n",
            "Epoch 3/4, Train Loss: 0.1189, Test Loss: 0.1006, Test Acc: 0.9695\n",
            "Epoch 4/4, Train Loss: 0.0936, Test Loss: 0.0874, Test Acc: 0.9728\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 69.0991\n",
            "Function value obtained: -0.9728\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.4451, Test Loss: 0.2297, Test Acc: 0.9317\n",
            "Epoch 2/3, Train Loss: 0.2224, Test Loss: 0.1643, Test Acc: 0.9513\n",
            "Epoch 3/3, Train Loss: 0.1708, Test Loss: 0.1314, Test Acc: 0.9607\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 46.7251\n",
            "Function value obtained: -0.9607\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.3928, Test Loss: 0.2064, Test Acc: 0.9417\n",
            "Epoch 2/3, Train Loss: 0.1903, Test Loss: 0.1454, Test Acc: 0.9578\n",
            "Epoch 3/3, Train Loss: 0.1388, Test Loss: 0.1106, Test Acc: 0.9679\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 47.1540\n",
            "Function value obtained: -0.9679\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 11 started. Searching for the next optimal point.\n",
            "Epoch 1/3, Train Loss: 0.2402, Test Loss: 0.1242, Test Acc: 0.9625\n",
            "Epoch 2/3, Train Loss: 0.1102, Test Loss: 0.1058, Test Acc: 0.9674\n",
            "Epoch 3/3, Train Loss: 0.0825, Test Loss: 0.0913, Test Acc: 0.9718\n",
            "Iteration No: 11 ended. Search finished for the next optimal point.\n",
            "Time taken: 53.7304\n",
            "Function value obtained: -0.9718\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 12 started. Searching for the next optimal point.\n",
            "Epoch 1/4, Train Loss: 0.2631, Test Loss: 0.1304, Test Acc: 0.9599\n",
            "Epoch 2/4, Train Loss: 0.1238, Test Loss: 0.0946, Test Acc: 0.9696\n",
            "Epoch 3/4, Train Loss: 0.0949, Test Loss: 0.0924, Test Acc: 0.9717\n",
            "Epoch 4/4, Train Loss: 0.0780, Test Loss: 0.0795, Test Acc: 0.9752\n",
            "Iteration No: 12 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.7340\n",
            "Function value obtained: -0.9752\n",
            "Current minimum: -0.9752\n",
            "Iteration No: 13 started. Searching for the next optimal point.\n",
            "Epoch 1/3, Train Loss: 0.2922, Test Loss: 0.1383, Test Acc: 0.9598\n",
            "Epoch 2/3, Train Loss: 0.1272, Test Loss: 0.1031, Test Acc: 0.9674\n",
            "Epoch 3/3, Train Loss: 0.0890, Test Loss: 0.0785, Test Acc: 0.9758\n",
            "Iteration No: 13 ended. Search finished for the next optimal point.\n",
            "Time taken: 44.0198\n",
            "Function value obtained: -0.9758\n",
            "Current minimum: -0.9758\n",
            "Iteration No: 14 started. Searching for the next optimal point.\n",
            "Epoch 1/3, Train Loss: 0.2706, Test Loss: 0.1453, Test Acc: 0.9528\n",
            "Epoch 2/3, Train Loss: 0.1832, Test Loss: 0.1143, Test Acc: 0.9648\n",
            "Epoch 3/3, Train Loss: 0.1601, Test Loss: 0.1237, Test Acc: 0.9660\n",
            "Iteration No: 14 ended. Search finished for the next optimal point.\n",
            "Time taken: 42.9045\n",
            "Function value obtained: -0.9660\n",
            "Current minimum: -0.9758\n",
            "Iteration No: 15 started. Searching for the next optimal point.\n",
            "Epoch 1/4, Train Loss: 0.2250, Test Loss: 0.1134, Test Acc: 0.9658\n",
            "Epoch 2/4, Train Loss: 0.1240, Test Loss: 0.1019, Test Acc: 0.9691\n",
            "Epoch 3/4, Train Loss: 0.1028, Test Loss: 0.0898, Test Acc: 0.9747\n",
            "Epoch 4/4, Train Loss: 0.0891, Test Loss: 0.1027, Test Acc: 0.9724\n",
            "Iteration No: 15 ended. Search finished for the next optimal point.\n",
            "Time taken: 68.4992\n",
            "Function value obtained: -0.9747\n",
            "Current minimum: -0.9758\n",
            "Best accuracy: 0.9758\n",
            "Best parameters: {'hidden_size': np.int64(245), 'dropout_rate': 0.10906815506053369, 'learning_rate': 0.0008393492851411112, 'batch_size': np.int64(7), 'epochs': np.int64(3)}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nBayesian Optimization\")\n",
        "\n",
        "from skopt.space import Integer, Real\n",
        "\n",
        "best_params, best_score = bayesian_optimization(\n",
        "    train_mnist,\n",
        "    [\n",
        "        Integer(64, 256, name='hidden_size'),\n",
        "        Real(0.1, 0.3, name='dropout_rate'),\n",
        "        Real(1e-4, 1e-2, prior='log-uniform', name='learning_rate'),\n",
        "        Integer(5, 7, name='batch_size'), # in powers of 2\n",
        "        Integer(3, 4 , name='epochs')\n",
        "    ],\n",
        "    n_calls=15\n",
        ")\n",
        "\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv8ez-bbB_Y0",
        "outputId": "a3ec30a7-cd7f-4c1d-b382-a9c1c80d4a5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " PSO\n",
            "Epoch 1/3, Train Loss: 0.2887, Test Loss: 0.1447, Test Acc: 0.9553\n",
            "Epoch 2/3, Train Loss: 0.2017, Test Loss: 0.1269, Test Acc: 0.9616\n",
            "Epoch 3/3, Train Loss: 0.1816, Test Loss: 0.1125, Test Acc: 0.9694\n",
            "Epoch 1/3, Train Loss: 0.2658, Test Loss: 0.1439, Test Acc: 0.9581\n",
            "Epoch 2/3, Train Loss: 0.1637, Test Loss: 0.1276, Test Acc: 0.9626\n",
            "Epoch 3/3, Train Loss: 0.1479, Test Loss: 0.1301, Test Acc: 0.9623\n",
            "Epoch 1/3, Train Loss: 0.2375, Test Loss: 0.1228, Test Acc: 0.9606\n",
            "Epoch 2/3, Train Loss: 0.1553, Test Loss: 0.1380, Test Acc: 0.9602\n",
            "Epoch 3/3, Train Loss: 0.1330, Test Loss: 0.1476, Test Acc: 0.9599\n",
            "Epoch 1/3, Train Loss: 0.4942, Test Loss: 0.2987, Test Acc: 0.9114\n",
            "Epoch 2/3, Train Loss: 0.4337, Test Loss: 0.2846, Test Acc: 0.9306\n",
            "Epoch 3/3, Train Loss: 0.4167, Test Loss: 0.2851, Test Acc: 0.9253\n",
            "Epoch 1/3, Train Loss: 0.6356, Test Loss: 0.3032, Test Acc: 0.9153\n",
            "Epoch 2/3, Train Loss: 0.3313, Test Loss: 0.2370, Test Acc: 0.9311\n",
            "Epoch 3/3, Train Loss: 0.2712, Test Loss: 0.2009, Test Acc: 0.9404\n",
            "Epoch 1/3, Train Loss: 0.4234, Test Loss: 0.2481, Test Acc: 0.9367\n",
            "Epoch 2/3, Train Loss: 0.3536, Test Loss: 0.3022, Test Acc: 0.9154\n",
            "Epoch 3/3, Train Loss: 0.3453, Test Loss: 0.3238, Test Acc: 0.9291\n",
            "Epoch 1/3, Train Loss: 0.2958, Test Loss: 0.1645, Test Acc: 0.9510\n",
            "Epoch 2/3, Train Loss: 0.2244, Test Loss: 0.1799, Test Acc: 0.9539\n",
            "Epoch 3/3, Train Loss: 0.2104, Test Loss: 0.1803, Test Acc: 0.9541\n",
            "Epoch 1/3, Train Loss: 0.2671, Test Loss: 0.1675, Test Acc: 0.9524\n",
            "Epoch 2/3, Train Loss: 0.1847, Test Loss: 0.1359, Test Acc: 0.9636\n",
            "Epoch 3/3, Train Loss: 0.1805, Test Loss: 0.1557, Test Acc: 0.9580\n",
            "Epoch 1/3, Train Loss: 0.2871, Test Loss: 0.1581, Test Acc: 0.9513\n",
            "Epoch 2/3, Train Loss: 0.2092, Test Loss: 0.1527, Test Acc: 0.9541\n",
            "Epoch 3/3, Train Loss: 0.1932, Test Loss: 0.1780, Test Acc: 0.9512\n",
            "Epoch 1/3, Train Loss: 0.3416, Test Loss: 0.1727, Test Acc: 0.9505\n",
            "Epoch 2/3, Train Loss: 0.1568, Test Loss: 0.1184, Test Acc: 0.9635\n",
            "Epoch 3/3, Train Loss: 0.1111, Test Loss: 0.0931, Test Acc: 0.9717\n",
            "Epoch 1/3, Train Loss: 0.2972, Test Loss: 0.1389, Test Acc: 0.9566\n",
            "Epoch 2/3, Train Loss: 0.2050, Test Loss: 0.1171, Test Acc: 0.9676\n",
            "Epoch 3/3, Train Loss: 0.1819, Test Loss: 0.1333, Test Acc: 0.9647\n",
            "Epoch 1/3, Train Loss: 0.2768, Test Loss: 0.1397, Test Acc: 0.9569\n",
            "Epoch 2/3, Train Loss: 0.1854, Test Loss: 0.1377, Test Acc: 0.9603\n",
            "Epoch 3/3, Train Loss: 0.1692, Test Loss: 0.1138, Test Acc: 0.9669\n",
            "Epoch 1/3, Train Loss: 0.2400, Test Loss: 0.1454, Test Acc: 0.9579\n",
            "Epoch 2/3, Train Loss: 0.1505, Test Loss: 0.1165, Test Acc: 0.9692\n",
            "Epoch 3/3, Train Loss: 0.1305, Test Loss: 0.1168, Test Acc: 0.9681\n",
            "Epoch 1/3, Train Loss: 0.3986, Test Loss: 0.2037, Test Acc: 0.9365\n",
            "Epoch 2/3, Train Loss: 0.3167, Test Loss: 0.1894, Test Acc: 0.9438\n",
            "Epoch 3/3, Train Loss: 0.3014, Test Loss: 0.1830, Test Acc: 0.9508\n",
            "Epoch 1/3, Train Loss: 0.3349, Test Loss: 0.1875, Test Acc: 0.9473\n",
            "Epoch 2/3, Train Loss: 0.2559, Test Loss: 0.1589, Test Acc: 0.9549\n",
            "Epoch 3/3, Train Loss: 0.2332, Test Loss: 0.1530, Test Acc: 0.9597\n",
            "Epoch 1/3, Train Loss: 0.4342, Test Loss: 0.2965, Test Acc: 0.9301\n",
            "Epoch 2/3, Train Loss: 0.3623, Test Loss: 0.2497, Test Acc: 0.9379\n",
            "Epoch 3/3, Train Loss: 0.3358, Test Loss: 0.2474, Test Acc: 0.9475\n",
            "Epoch 1/3, Train Loss: 0.2992, Test Loss: 0.1862, Test Acc: 0.9436\n",
            "Epoch 2/3, Train Loss: 0.2244, Test Loss: 0.1635, Test Acc: 0.9553\n",
            "Epoch 3/3, Train Loss: 0.2143, Test Loss: 0.1662, Test Acc: 0.9572\n",
            "Epoch 1/3, Train Loss: 0.3656, Test Loss: 0.2373, Test Acc: 0.9340\n",
            "Epoch 2/3, Train Loss: 0.2935, Test Loss: 0.2372, Test Acc: 0.9438\n",
            "Epoch 3/3, Train Loss: 0.2871, Test Loss: 0.2225, Test Acc: 0.9468\n",
            "Epoch 1/3, Train Loss: 0.3752, Test Loss: 0.2355, Test Acc: 0.9373\n",
            "Epoch 2/3, Train Loss: 0.3016, Test Loss: 0.2388, Test Acc: 0.9394\n",
            "Epoch 3/3, Train Loss: 0.2924, Test Loss: 0.2320, Test Acc: 0.9400\n",
            "Epoch 1/3, Train Loss: 0.2519, Test Loss: 0.1178, Test Acc: 0.9619\n",
            "Epoch 2/3, Train Loss: 0.1138, Test Loss: 0.0857, Test Acc: 0.9739\n",
            "Epoch 3/3, Train Loss: 0.0831, Test Loss: 0.0785, Test Acc: 0.9770\n",
            "Epoch 1/3, Train Loss: 0.3722, Test Loss: 0.2972, Test Acc: 0.9173\n",
            "Epoch 2/3, Train Loss: 0.3075, Test Loss: 0.2510, Test Acc: 0.9363\n",
            "Epoch 3/3, Train Loss: 0.2834, Test Loss: 0.2267, Test Acc: 0.9443\n",
            "Epoch 1/3, Train Loss: 0.4039, Test Loss: 0.2711, Test Acc: 0.9305\n",
            "Epoch 2/3, Train Loss: 0.3574, Test Loss: 0.2660, Test Acc: 0.9289\n",
            "Epoch 3/3, Train Loss: 0.3397, Test Loss: 0.2893, Test Acc: 0.9365\n",
            "Epoch 1/3, Train Loss: 0.3709, Test Loss: 0.2437, Test Acc: 0.9336\n",
            "Epoch 2/3, Train Loss: 0.3138, Test Loss: 0.2439, Test Acc: 0.9414\n",
            "Epoch 3/3, Train Loss: 0.2952, Test Loss: 0.2075, Test Acc: 0.9462\n",
            "Epoch 1/3, Train Loss: 0.4077, Test Loss: 0.3196, Test Acc: 0.9235\n",
            "Epoch 2/3, Train Loss: 0.3584, Test Loss: 0.2453, Test Acc: 0.9406\n",
            "Epoch 3/3, Train Loss: 0.3407, Test Loss: 0.3193, Test Acc: 0.9317\n",
            "Epoch 1/3, Train Loss: 0.3743, Test Loss: 0.2721, Test Acc: 0.9294\n",
            "Epoch 2/3, Train Loss: 0.3012, Test Loss: 0.2013, Test Acc: 0.9437\n",
            "Epoch 3/3, Train Loss: 0.2786, Test Loss: 0.1985, Test Acc: 0.9485\n",
            "Epoch 1/3, Train Loss: 0.4116, Test Loss: 0.2476, Test Acc: 0.9308\n",
            "Epoch 2/3, Train Loss: 0.3562, Test Loss: 0.2508, Test Acc: 0.9325\n",
            "Epoch 3/3, Train Loss: 0.3360, Test Loss: 0.2789, Test Acc: 0.9402\n",
            "Epoch 1/3, Train Loss: 0.4012, Test Loss: 0.2812, Test Acc: 0.9215\n",
            "Epoch 2/3, Train Loss: 0.3418, Test Loss: 0.2429, Test Acc: 0.9405\n",
            "Epoch 3/3, Train Loss: 0.3388, Test Loss: 0.2528, Test Acc: 0.9396\n",
            "Epoch 1/3, Train Loss: 0.4307, Test Loss: 0.2732, Test Acc: 0.9209\n",
            "Epoch 2/3, Train Loss: 0.3763, Test Loss: 0.2912, Test Acc: 0.9234\n",
            "Epoch 3/3, Train Loss: 0.3552, Test Loss: 0.2683, Test Acc: 0.9254\n",
            "Epoch 1/3, Train Loss: 0.4226, Test Loss: 0.2914, Test Acc: 0.9228\n",
            "Epoch 2/3, Train Loss: 0.3721, Test Loss: 0.3360, Test Acc: 0.9249\n",
            "Epoch 3/3, Train Loss: 0.3523, Test Loss: 0.3308, Test Acc: 0.9176\n",
            "Epoch 1/3, Train Loss: 0.3532, Test Loss: 0.2165, Test Acc: 0.9386\n",
            "Epoch 2/3, Train Loss: 0.2971, Test Loss: 0.2728, Test Acc: 0.9262\n",
            "Epoch 3/3, Train Loss: 0.2791, Test Loss: 0.2361, Test Acc: 0.9433\n",
            "Epoch 1/3, Train Loss: 0.4070, Test Loss: 0.2834, Test Acc: 0.9202\n",
            "Epoch 2/3, Train Loss: 0.3396, Test Loss: 0.2562, Test Acc: 0.9370\n",
            "Epoch 3/3, Train Loss: 0.3259, Test Loss: 0.2602, Test Acc: 0.9379\n",
            "Epoch 1/3, Train Loss: 0.3989, Test Loss: 0.2685, Test Acc: 0.9241\n",
            "Epoch 2/3, Train Loss: 0.3479, Test Loss: 0.2728, Test Acc: 0.9269\n",
            "Epoch 3/3, Train Loss: 0.3338, Test Loss: 0.2797, Test Acc: 0.9338\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n PSO\")\n",
        "best_params, best_score, results = PSO_hyperparam(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb7dv5CPCkCH"
      },
      "outputs": [],
      "source": [
        "print(\"\\n RL PSO\")\n",
        "best_params, best_score, results = RL_PSO_hyperparam(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX9iMZ79CtXA"
      },
      "outputs": [],
      "source": [
        "param_grid_cifar10 = {\n",
        "    'filters1': [32, 64, 128],\n",
        "    'filters2': [64, 128, 256],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'epochs': 3,  # Fixed for faster execution\n",
        "    'optimizer_type': ['adam', 'sgd']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5af3xCBCl7e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nExample 2: CIFAR-10\")\n",
        "print(\"\\nRandom Search\")\n",
        "best_params, best_score, results = random_search(train_cifar10, param_grid_cifar10, n_trials=5)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kgb9zn0C0um"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Grid Search\")\n",
        "best_params, best_score, results = grid_search(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjDNcseiYRcl"
      },
      "outputs": [],
      "source": [
        "print(\"\\nBayesian Optimization\")\n",
        "from skopt.space import Real, Integer\n",
        "param_space_cifar10 = [\n",
        "    Integer(32, 128, name='filters1'),\n",
        "    Integer(64, 256, name='filters2'),\n",
        "    Real(0.1, 0.3, name='dropout_rate'),\n",
        "    Real(1e-4, 1e-2, prior='log-uniform', name='learning_rate'),\n",
        "    Integer(32, 128, name='batch_size'),\n",
        "    Integer(3, 10, name='epochs')\n",
        "]\n",
        "\n",
        "best_params, best_score = bayesian_optimization(\n",
        "    train_cifar10,\n",
        "    param_space_cifar10,\n",
        "    n_calls=5\n",
        ")\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1Pk40XAC4gQ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n PSO\")\n",
        "best_params, best_score, results = PSO(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiWqaejRC7RG"
      },
      "outputs": [],
      "source": [
        "print(\"\\n RL PSO\")\n",
        "best_params, best_score, results = RL_PSO(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}