{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s0han24/NA_RLPSO/blob/main/NA_RL_PSO_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from torch) (72.1.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from torchvision) (2.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from scikit-learn) (1.15.1)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from scikit-optimize) (24.2)\n",
            "Collecting PyYAML (from pyaml>=16.9->scikit-optimize)\n",
            "  Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
            "Downloading torch-2.7.0-cp313-none-macosx_11_0_arm64.whl (68.6 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "Downloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
            "Installing collected packages: mpmath, threadpoolctl, sympy, PyYAML, networkx, MarkupSafe, joblib, fsspec, filelock, scikit-learn, pyaml, jinja2, torch, scikit-optimize, torchvision\n",
            "Successfully installed MarkupSafe-3.0.2 PyYAML-6.0.2 filelock-3.18.0 fsspec-2025.3.2 jinja2-3.1.6 joblib-1.5.0 mpmath-1.3.0 networkx-3.4.2 pyaml-25.1.0 scikit-learn-1.6.1 scikit-optimize-0.10.2 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.7.0 torchvision-0.22.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision scikit-learn scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.67.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o1EJufEb42Mj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.datasets import load_digits, load_wine, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JRtB1CSO45AQ"
      },
      "outputs": [],
      "source": [
        "# Define objective function to minimize (Rastrigin function)\n",
        "def rastrigin(x):\n",
        "    A = 10\n",
        "    return A * len(x) + sum([(xi ** 2 - A * np.cos(2 * np.pi * xi)) for xi in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rn_BWhFM5tDw"
      },
      "outputs": [],
      "source": [
        "# Particle Swarm Optimization implementation (baseline)\n",
        "class Particle:\n",
        "    def __init__(self, dim, bounds, objective=rastrigin, random_sequence=None):\n",
        "        # replace random with some sequence for experimentation\n",
        "        if random_sequence is not None:\n",
        "            self.position = random_sequence\n",
        "        else:\n",
        "            self.position = np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)])\n",
        "        self.velocity = np.zeros(dim)\n",
        "        self.best_position = np.copy(self.position)\n",
        "        self.best_value = objective(self.position)\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin, random_sequences=None):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.num_particles = num_particles\n",
        "        self.num_iter = num_iter\n",
        "        self.objective = objective\n",
        "        if random_sequences is None:\n",
        "            self.swarm = [Particle(dim, bounds, objective) for _ in range(num_particles)]\n",
        "        else :\n",
        "            self.swarm = [Particle(dim, bounds, objective, random_sequence) for random_sequence in random_sequences]\n",
        "        self.global_best_position = np.copy(self.swarm[0].position)\n",
        "        self.global_best_value = objective(self.global_best_position)\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "        c1, c2 = 1.5, 1.5\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "        for _ in range(self.num_iter):\n",
        "            X = []\n",
        "            V = []\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:, 0], self.bounds[:, 1])\n",
        "                value = self.objective(particle.position)\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SKfEtN_r4y_r"
      },
      "outputs": [],
      "source": [
        "# Policy Network for the RL component\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.action_mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Learnable log_std parameter for the Gaussian distribution\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mean = self.action_mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "    def select_action(self, state):\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum()  # Sum log probabilities across dimensions\n",
        "        # Clamp actions to a plausible range for c1 and c2 (e.g., [1.0, 2.0])\n",
        "        action = torch.clamp(action, 1.0, 2.0)\n",
        "        return action, log_prob\n",
        "\n",
        "# RL-enhanced PSO using the policy network to select acceleration coefficients c1 and c2\n",
        "class RLPSO(PSO):\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin, lr=1e-2, random_sequences=None):\n",
        "        super().__init__(dim, bounds, num_particles, num_iter, objective, random_sequences)\n",
        "        # Define a simple state: [iteration_ratio, global_best_value]\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 2  # one for c1 and one for c2\n",
        "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.objective = objective\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def get_state(self, iteration):\n",
        "        # Normalize iteration to [0, 1]\n",
        "        iter_norm = iteration / self.num_iter\n",
        "        # Use the current global best value (optionally, further normalization can be applied)\n",
        "        state = np.array([iter_norm, self.global_best_value], dtype=np.float32)\n",
        "        return torch.tensor(state)\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "\n",
        "        for it in tqdm(range(1, self.num_iter + 1), desc=\"Training progress\"):\n",
        "            state = self.get_state(it)\n",
        "            action, log_prob = self.policy_net.select_action(state)\n",
        "            c1, c2 = action.detach().numpy()  # use the selected coefficients for this iteration\n",
        "            X = []\n",
        "            V = []\n",
        "            # Store the old global best for reward computation\n",
        "            old_global_best = self.global_best_value\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:,0], self.bounds[:,1])\n",
        "                value = self.objective(particle.position)\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "\n",
        "            # Reward: improvement in global best value\n",
        "            reward = old_global_best - self.global_best_value\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            # Update policy network using the REINFORCE rule: maximize reward\n",
        "            loss = -log_prob * reward_tensor\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hcksn8R5xh8",
        "outputId": "42c9287a-121a-4cbc-8364-b4d9f65cfd5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running baseline PSO...\n",
            "\n",
            "Baseline PSO solution:\n",
            "Best position: [9.38081517e-07 1.18304081e-07]\n",
            "Best value: 1.773585722730786e-10\n",
            "\n",
            "Running RL-enhanced PSO (RLPSO)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training progress:   0%|          | 0/30 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training progress: 100%|██████████| 30/30 [00:00<00:00, 411.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RLPSO solution:\n",
            "Best position: [1.63840092e-05 2.44843325e-05]\n",
            "Best value: 1.7218812331520894e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Testing both the baseline PSO and the RL-enhanced PSO\n",
        "dim = 2\n",
        "bounds = np.array([[-2,2] for _ in range(dim)])\n",
        "num_particles = 40\n",
        "num_iter = 30\n",
        "\n",
        "# generate num_particles number of random sequences of length dim each\n",
        "random_sequences = [np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)]) for _ in range(num_particles)]\n",
        "\n",
        "\n",
        "print(\"Running baseline PSO...\")\n",
        "pso_solver = PSO(dim, bounds, num_particles, num_iter, random_sequences=random_sequences)\n",
        "best_position_pso, best_value_pso, X_list_pso, V_list_pso, pso_results = pso_solver.optimize()\n",
        "print(\"\\nBaseline PSO solution:\")\n",
        "print(\"Best position:\", best_position_pso)\n",
        "print(\"Best value:\", best_value_pso)\n",
        "\n",
        "print(\"\\nRunning RL-enhanced PSO (RLPSO)...\")\n",
        "rl_pso_solver = RLPSO(dim, bounds, num_particles, num_iter, random_sequences=random_sequences)\n",
        "best_position_rl, best_value_rl, X_list, V_list, rl_pso_results = rl_pso_solver.optimize()\n",
        "print(\"\\nRLPSO solution:\")\n",
        "print(\"Best position:\", best_position_rl)\n",
        "print(\"Best value:\", best_value_rl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1vtQOMmrcvVv",
        "outputId": "4d8a3e7e-339a-469c-e3d6-278ccd78e375"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# prompt: plot the history from the pso algorithms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the history of the global best value for both PSO algorithms\n",
        "plt.plot(pso_results, label='PSO')\n",
        "plt.plot(rl_pso_results, label='RL-PSO')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Global Best Value')\n",
        "plt.title('Convergence History')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mcMtxh9X1Q_",
        "outputId": "e9d22c07-a59d-41f4-a01a-17d5d06adafd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-opt\n",
            "  Downloading scikit_opt-0.6.6-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from scikit-opt) (2.2.2)\n",
            "Requirement already satisfied: scipy in /Users/sohanthuumala/miniconda3/envs/na/lib/python3.13/site-packages (from scikit-opt) (1.15.1)\n",
            "Downloading scikit_opt-0.6.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: scikit-opt\n",
            "Successfully installed scikit-opt-0.6.6\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv0HrojN_qQ2"
      },
      "source": [
        "## Training some simple models to test hyperparameter tuning capabilities\n",
        "\n",
        "#### Model 1: Simple MLP for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "biKxefM8_pOO"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, hidden_size=128, dropout_rate=0.2):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tom0MLr4_zlN"
      },
      "outputs": [],
      "source": [
        "def train_mnist(\n",
        "    hidden_size=128,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    weight_decay=0.0,\n",
        "    momentum=0.9,\n",
        "    optimizer_type='adam'\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a simple MLP on MNIST dataset\n",
        "\n",
        "    Parameters:\n",
        "    - hidden_size: number of neurons in the hidden layer\n",
        "    - dropout_rate: dropout probability\n",
        "    - learning_rate: learning rate for the optimizer\n",
        "    - batch_size: size of mini-batches\n",
        "    - epochs: number of training epochs\n",
        "    - weight_decay: L2 regularization parameter\n",
        "    - momentum: momentum parameter (for SGD)\n",
        "    - optimizer_type: 'adam' or 'sgd'\n",
        "\n",
        "    Returns:\n",
        "    - trained model\n",
        "    - dictionary with training history\n",
        "    \"\"\"\n",
        "    # Data loading\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleMLP(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = correct / total\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "082TtNIzAG44"
      },
      "source": [
        "#### Model 2: CNN for CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cZh5ExCN_7yM"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, filters1=32, filters2=64, dropout_rate=0.25):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, filters1, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(filters1)\n",
        "        self.conv2 = nn.Conv2d(filters1, filters2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(filters2)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(filters2 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lwo5_IKfAVC6"
      },
      "outputs": [],
      "source": [
        "def train_cifar10(\n",
        "    filters1=32,\n",
        "    filters2=64,\n",
        "    dropout_rate=0.25,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    epochs=10,\n",
        "    weight_decay=0.0001,\n",
        "    optimizer_type='adam'\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a simple CNN on CIFAR-10 dataset\n",
        "\n",
        "    Parameters:\n",
        "    - filters1: number of filters in first conv layer\n",
        "    - filters2: number of filters in second conv layer\n",
        "    - dropout_rate: dropout probability\n",
        "    - learning_rate: learning rate for the optimizer\n",
        "    - batch_size: size of mini-batches\n",
        "    - epochs: number of training epochs\n",
        "    - weight_decay: L2 regularization parameter\n",
        "    - optimizer_type: 'adam' or 'sgd'\n",
        "\n",
        "    Returns:\n",
        "    - trained model\n",
        "    - dictionary with training history\n",
        "    \"\"\"\n",
        "    # Data loading\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleCNN(filters1=filters1, filters2=filters2, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "\n",
        "    # Training loop\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = correct / total\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(test_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQFo-7gAl3s"
      },
      "source": [
        "## Baseline methods for hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b0Ors2tzAahr"
      },
      "outputs": [],
      "source": [
        "def random_search(model_func, param_grid, n_trials=10):\n",
        "    \"\"\"\n",
        "    Perform random search for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible values\n",
        "    - n_trials: number of random combinations to try\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "\n",
        "    for _ in tqdm(range(n_trials)):\n",
        "        # Sample random hyperparameters\n",
        "        params = {k: random.choice(v) if isinstance(v, list) else v for k, v in param_grid.items()}\n",
        "        params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "\n",
        "        # Train model with sampled hyperparameters\n",
        "        _, history = model_func(**params)\n",
        "\n",
        "        # Use the best test accuracy as the score\n",
        "        score = max(history['test_acc'])\n",
        "\n",
        "        results.append((params, score))\n",
        "\n",
        "        # Update best parameters if needed\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "T2mTAKBbAytC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def grid_search(model_func, param_grid):\n",
        "    \"\"\"\n",
        "    Perform grid search for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible values\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate all combinations of parameters\n",
        "    keys = param_grid.keys()\n",
        "\n",
        "    # Make sure all values are iterable (convert single values to lists)\n",
        "    values = []\n",
        "    for key in keys:\n",
        "        if isinstance(param_grid[key], (list, tuple)):\n",
        "            values.append(param_grid[key])\n",
        "        else:\n",
        "            # If the parameter is a single value (like an integer), wrap it in a list\n",
        "            values.append([param_grid[key]])\n",
        "\n",
        "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    results = []\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "\n",
        "    for params in tqdm(param_combinations):\n",
        "        # Train model with parameters\n",
        "        _, history = model_func(**params)\n",
        "\n",
        "        # Use the best test accuracy as the score\n",
        "        score = max(history['test_acc'])\n",
        "\n",
        "        results.append((params, score))\n",
        "\n",
        "        # Update best parameters if needed\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA-hwhSjCOSt"
      },
      "outputs": [],
      "source": [
        "def PSO(model_func, param_grid, num_particles=30, num_iter=10, w=0.7, c1=2, c2=2):\n",
        "    pso_solver = PSO(dim=len(param_grid), bounds=np.array([[min(v), max(v)] for v in param_grid.values()]), num_particles=num_particles, num_iter=num_iter, objective=model_func)\n",
        "\n",
        "    best_position, best_value, X_list, V_list, pso_results = pso_solver.optimize()\n",
        "    best_params = {k: v for k, v in zip(param_grid.keys(), best_position)}\n",
        "    return best_params, best_value, pso_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AT9IWIuKCAdl"
      },
      "outputs": [],
      "source": [
        "def RL_PSO(model_func, param_grid, num_particles=30, num_iter=10):\n",
        "    rl_pso_solver = RLPSO(dim=len(param_grid), bounds=np.array([[min(v), max(v)] for v in param_grid.values()]), num_particles=num_particles, num_iter=num_iter, objective=model_func)\n",
        "    best_position, best_value, X_list, V_list, rl_pso_results = rl_pso_solver.optimize()\n",
        "    best_params = {k: v for k, v in zip(param_grid.keys(), best_position)}\n",
        "    return best_params, best_value, rl_pso_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from skopt import gp_minimize\n",
        "\n",
        "def bayesian_optimization(model_func, param_space, n_calls=10):\n",
        "    \"\"\"\n",
        "    Perform Bayesian Optimization for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_space: list of skopt.space dimensions defining the search space\n",
        "    - n_calls: number of iterations for optimization\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the objective function (minimize negative accuracy)\n",
        "    def objective(params):\n",
        "        param_dict = {dim.name: param for dim, param in zip(param_space, params)}\n",
        "        param_dict['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "        _, history = model_func(**param_dict)\n",
        "        return -max(history['test_acc'])  # Return negative accuracy for minimization\n",
        "\n",
        "    # Run Bayesian optimization\n",
        "    result = gp_minimize(\n",
        "        objective,\n",
        "        param_space,\n",
        "        n_calls=n_calls,\n",
        "        random_state=42,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Extract the best parameters\n",
        "    best_params = {dim.name: result.x[i] for i, dim in enumerate(param_space)}\n",
        "    best_score = -result.fun\n",
        "\n",
        "    return best_params, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Qhp9zi5ZA5q2"
      },
      "outputs": [],
      "source": [
        "param_grid_mnist = {\n",
        "    'hidden_size': [64, 128, 256],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'epochs': 3,  # Fixed for faster execution\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yNPWvBA-A9EB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1: MNIST\n",
            "\n",
            "Random Search\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:36<00:00, 274kB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 110kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:02<00:00, 623kB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.21MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Train Loss: 0.3817, Test Loss: 0.1734, Test Acc: 0.9459\n",
            "Epoch 2/3, Train Loss: 0.2193, Test Loss: 0.1303, Test Acc: 0.9613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [01:01<04:04, 61.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1854, Test Loss: 0.1218, Test Acc: 0.9643\n",
            "Epoch 1/3, Train Loss: 0.3149, Test Loss: 0.1802, Test Acc: 0.9459\n",
            "Epoch 2/3, Train Loss: 0.2180, Test Loss: 0.1512, Test Acc: 0.9553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [01:14<01:39, 33.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1911, Test Loss: 0.1497, Test Acc: 0.9571\n",
            "Epoch 1/3, Train Loss: 0.3935, Test Loss: 0.2086, Test Acc: 0.9377\n",
            "Epoch 2/3, Train Loss: 0.3222, Test Loss: 0.2054, Test Acc: 0.9410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [01:29<00:49, 24.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3117, Test Loss: 0.2313, Test Acc: 0.9391\n",
            "Epoch 1/3, Train Loss: 0.2984, Test Loss: 0.1523, Test Acc: 0.9521\n",
            "Epoch 2/3, Train Loss: 0.1572, Test Loss: 0.1190, Test Acc: 0.9633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [01:46<00:21, 21.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1231, Test Loss: 0.0967, Test Acc: 0.9697\n",
            "Epoch 1/3, Train Loss: 0.4364, Test Loss: 0.2191, Test Acc: 0.9324\n",
            "Epoch 2/3, Train Loss: 0.3360, Test Loss: 0.1956, Test Acc: 0.9445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [02:00<00:00, 24.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3025, Test Loss: 0.1693, Test Acc: 0.9513\n",
            "Best accuracy: 0.9697\n",
            "Best parameters: {'hidden_size': 64, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 3, 'optimizer_type': 'adam'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nExample 1: MNIST\")\n",
        "print(\"\\nRandom Search\")\n",
        "best_params, best_score, results = random_search(train_mnist, param_grid_mnist, n_trials=5)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FVn3UHHmBE4F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Grid Search\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/81 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Train Loss: 0.6129, Test Loss: 0.3067, Test Acc: 0.9130\n",
            "Epoch 2/3, Train Loss: 0.3069, Test Loss: 0.2371, Test Acc: 0.9316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 1/81 [00:16<21:54, 16.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2533, Test Loss: 0.2086, Test Acc: 0.9375\n",
            "Epoch 1/3, Train Loss: 0.7547, Test Loss: 0.3561, Test Acc: 0.9054\n",
            "Epoch 2/3, Train Loss: 0.3528, Test Loss: 0.2784, Test Acc: 0.9215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/81 [00:30<20:06, 15.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2945, Test Loss: 0.2427, Test Acc: 0.9309\n",
            "Epoch 1/3, Train Loss: 0.9249, Test Loss: 0.4373, Test Acc: 0.8902\n",
            "Epoch 2/3, Train Loss: 0.4169, Test Loss: 0.3257, Test Acc: 0.9097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▎         | 3/81 [00:44<18:55, 14.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3417, Test Loss: 0.2830, Test Acc: 0.9192\n",
            "Epoch 1/3, Train Loss: 0.2877, Test Loss: 0.1493, Test Acc: 0.9559\n",
            "Epoch 2/3, Train Loss: 0.1491, Test Loss: 0.1155, Test Acc: 0.9656\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 4/81 [01:01<19:51, 15.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1193, Test Loss: 0.1081, Test Acc: 0.9680\n",
            "Epoch 1/3, Train Loss: 0.3234, Test Loss: 0.1586, Test Acc: 0.9535\n",
            "Epoch 2/3, Train Loss: 0.1630, Test Loss: 0.1263, Test Acc: 0.9623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 5/81 [01:16<19:21, 15.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1304, Test Loss: 0.1007, Test Acc: 0.9719\n",
            "Epoch 1/3, Train Loss: 0.3784, Test Loss: 0.1931, Test Acc: 0.9437\n",
            "Epoch 2/3, Train Loss: 0.1876, Test Loss: 0.1434, Test Acc: 0.9573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 6/81 [01:30<18:25, 14.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1454, Test Loss: 0.1122, Test Acc: 0.9679\n",
            "Epoch 1/3, Train Loss: 0.4135, Test Loss: 0.2815, Test Acc: 0.9244\n",
            "Epoch 2/3, Train Loss: 0.3570, Test Loss: 0.2395, Test Acc: 0.9330\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 7/81 [01:47<19:08, 15.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3321, Test Loss: 0.2722, Test Acc: 0.9391\n",
            "Epoch 1/3, Train Loss: 0.3876, Test Loss: 0.2346, Test Acc: 0.9339\n",
            "Epoch 2/3, Train Loss: 0.2951, Test Loss: 0.2131, Test Acc: 0.9357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▊         | 7/81 [02:01<21:24, 17.36s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Grid Search\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_params, best_score, results = \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_mnist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid_mnist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mgrid_search\u001b[39m\u001b[34m(model_func, param_grid)\u001b[39m\n\u001b[32m     31\u001b[39m best_params = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m tqdm(param_combinations):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Train model with parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     _, history = \u001b[43mmodel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Use the best test accuracy as the score\u001b[39;00m\n\u001b[32m     38\u001b[39m     score = \u001b[38;5;28mmax\u001b[39m(history[\u001b[33m'\u001b[39m\u001b[33mtest_acc\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mtrain_mnist\u001b[39m\u001b[34m(hidden_size, dropout_rate, learning_rate, batch_size, epochs, weight_decay, momentum, optimizer_type)\u001b[39m\n\u001b[32m     91\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/na/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/na/lib/python3.13/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/na/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/na/lib/python3.13/site-packages/torchvision/datasets/mnist.py:143\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    139\u001b[39m img, target = \u001b[38;5;28mself\u001b[39m.data[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.targets[index])\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    146\u001b[39m     img = \u001b[38;5;28mself\u001b[39m.transform(img)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/na/lib/python3.13/site-packages/PIL/Image.py:3338\u001b[39m, in \u001b[36mfromarray\u001b[39m\u001b[34m(obj, mode)\u001b[39m\n\u001b[32m   3335\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstrides\u001b[39m\u001b[33m'\u001b[39m\u001b[33m requires either tobytes() or tostring()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3336\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m3338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/na/lib/python3.13/site-packages/PIL/Image.py:3231\u001b[39m, in \u001b[36mfrombuffer\u001b[39m\u001b[34m(mode, size, data, decoder_name, *args)\u001b[39m\n\u001b[32m   3229\u001b[39m     args = mode, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m\n\u001b[32m   3230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m _MAPMODES:\n\u001b[32m-> \u001b[39m\u001b[32m3231\u001b[39m     im = \u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3232\u001b[39m     im = im._new(core.map_buffer(data, size, decoder_name, \u001b[32m0\u001b[39m, args))\n\u001b[32m   3233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/na/lib/python3.13/site-packages/PIL/Image.py:3132\u001b[39m, in \u001b[36mnew\u001b[39m\u001b[34m(mode, size, color)\u001b[39m\n\u001b[32m   3130\u001b[39m         im.palette = ImagePalette.ImagePalette()\n\u001b[32m   3131\u001b[39m         color = im.palette.getcolor(color_ints)\n\u001b[32m-> \u001b[39m\u001b[32m3132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m im._new(\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(\"\\n Grid Search\")\n",
        "best_params, best_score, results = grid_search(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "_grTOLQIBIBV",
        "outputId": "53be4a31-4f38-461d-bdc7-e12335746a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bayesian Optimization\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'bayesian_optimization_example' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-504b2b1acef6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBayesian Optimization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayesian_optimization_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best accuracy: {best_score:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters: {best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bayesian_optimization_example' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\nBayesian Optimization\")\n",
        "\n",
        "from skopt.space import Integer, Real\n",
        "\n",
        "best_params, best_score = bayesian_optimization(\n",
        "    train_mnist,\n",
        "    [\n",
        "        Integer(64, 256, name='hidden_size'),\n",
        "        Real(0.1, 0.3, name='dropout_rate'),\n",
        "        Real(1e-4, 1e-2, prior='log-uniform', name='learning_rate'),\n",
        "        Integer(32, 128, name='batch_size'),\n",
        "        Integer(3, 10, name='epochs')\n",
        "    ],\n",
        "    n_calls=5\n",
        ")\n",
        "\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv8ez-bbB_Y0"
      },
      "outputs": [],
      "source": [
        "print(\"\\n PSO\")\n",
        "best_params, best_score, results = PSO(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb7dv5CPCkCH"
      },
      "outputs": [],
      "source": [
        "print(\"\\n RL PSO\")\n",
        "best_params, best_score, results = RL_PSO(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX9iMZ79CtXA"
      },
      "outputs": [],
      "source": [
        "param_grid_cifar10 = {\n",
        "    'filters1': [32, 64, 128],\n",
        "    'filters2': [64, 128, 256],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'epochs': 3,  # Fixed for faster execution\n",
        "    'optimizer_type': ['adam', 'sgd']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5af3xCBCl7e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nExample 2: CIFAR-10\")\n",
        "print(\"\\nRandom Search\")\n",
        "best_params, best_score, results = random_search(train_cifar10, param_grid_cifar10, n_trials=5)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kgb9zn0C0um"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Grid Search\")\n",
        "best_params, best_score, results = grid_search(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nBayesian Optimization\")\n",
        "from skopt.space import Real, Integer\n",
        "param_space_cifar10 = [\n",
        "    Integer(32, 128, name='filters1'),\n",
        "    Integer(64, 256, name='filters2'),\n",
        "    Real(0.1, 0.3, name='dropout_rate'),\n",
        "    Real(1e-4, 1e-2, prior='log-uniform', name='learning_rate'),\n",
        "    Integer(32, 128, name='batch_size'),\n",
        "    Integer(3, 10, name='epochs')\n",
        "]\n",
        "\n",
        "best_params, best_score = bayesian_optimization(\n",
        "    train_cifar10,\n",
        "    param_space_cifar10,\n",
        "    n_calls=5\n",
        ")\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1Pk40XAC4gQ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n PSO\")\n",
        "best_params, best_score, results = PSO(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiWqaejRC7RG"
      },
      "outputs": [],
      "source": [
        "print(\"\\n RL PSO\")\n",
        "best_params, best_score, results = RL_PSO(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "na",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
