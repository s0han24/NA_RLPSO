{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s0han24/NA_RLPSO/blob/main/NA_RL_PSO_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNiYfjRPYRcT",
        "outputId": "51f7c294-1408-4dff-86ad-287e3804266b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.1.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.1.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, scikit-optimize, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyaml-25.1.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision scikit-learn scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlSwj7y2YRcV",
        "outputId": "1a160ba2-2b89-4ad2-ed65-12a1b24dae28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o1EJufEb42Mj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.datasets import load_digits, load_wine, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v8J-enbOYRcW"
      },
      "outputs": [],
      "source": [
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JRtB1CSO45AQ"
      },
      "outputs": [],
      "source": [
        "# Define objective function to minimize (Rastrigin function)\n",
        "def rastrigin(x):\n",
        "    A = 10\n",
        "    return A * len(x) + sum([(xi ** 2 - A * np.cos(2 * np.pi * xi)) for xi in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rn_BWhFM5tDw"
      },
      "outputs": [],
      "source": [
        "# Particle Swarm Optimization implementation (baseline)\n",
        "class Particle:\n",
        "    def __init__(self, dim, bounds, objective=rastrigin, random_sequence=None):\n",
        "        # replace random with some sequence for experimentation\n",
        "        if random_sequence is not None:\n",
        "            self.position = random_sequence\n",
        "        else:\n",
        "            self.position = np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)])\n",
        "        self.velocity = np.zeros(dim)\n",
        "        self.best_position = np.copy(self.position)\n",
        "        self.best_value = objective(self.position)\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin, random_sequences=None):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.num_particles = num_particles\n",
        "        self.num_iter = num_iter\n",
        "        self.objective = objective\n",
        "        if random_sequences is None:\n",
        "            self.swarm = [Particle(dim, bounds, objective) for _ in range(num_particles)]\n",
        "        else :\n",
        "            self.swarm = [Particle(dim, bounds, objective, random_sequence) for random_sequence in random_sequences]\n",
        "        self.global_best_position = np.copy(self.swarm[0].position)\n",
        "        self.global_best_value = objective(self.global_best_position)\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "        c1, c2 = 1.5, 1.5\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "        for _ in range(self.num_iter):\n",
        "            X = []\n",
        "            V = []\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:, 0], self.bounds[:, 1])\n",
        "                value = self.objective(particle.position)\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SKfEtN_r4y_r"
      },
      "outputs": [],
      "source": [
        "# Policy Network for the RL component\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.action_mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Learnable log_std parameter for the Gaussian distribution\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mean = self.action_mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "    def select_action(self, state):\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum()  # Sum log probabilities across dimensions\n",
        "        # Clamp actions to a plausible range for c1 and c2 (e.g., [1.0, 2.0])\n",
        "        action = torch.clamp(action, 1.0, 2.0)\n",
        "        return action, log_prob\n",
        "\n",
        "# RL-enhanced PSO using the policy network to select acceleration coefficients c1 and c2\n",
        "class RLPSO(PSO):\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin, lr=1e-2, random_sequences=None):\n",
        "        super().__init__(dim, bounds, num_particles, num_iter, objective, random_sequences)\n",
        "        # Define a simple state: [iteration_ratio, global_best_value]\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 2  # one for c1 and one for c2\n",
        "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.objective = objective\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def get_state(self, iteration):\n",
        "        # Normalize iteration to [0, 1]\n",
        "        iter_norm = iteration / self.num_iter\n",
        "        # Use the current global best value (optionally, further normalization can be applied)\n",
        "        state = np.array([iter_norm, self.global_best_value], dtype=np.float32)\n",
        "        return torch.tensor(state)\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "\n",
        "        for it in tqdm(range(1, self.num_iter + 1), desc=\"Training progress\"):\n",
        "            state = self.get_state(it)\n",
        "            action, log_prob = self.policy_net.select_action(state)\n",
        "            c1, c2 = action.detach().numpy()  # use the selected coefficients for this iteration\n",
        "            X = []\n",
        "            V = []\n",
        "            # Store the old global best for reward computation\n",
        "            old_global_best = self.global_best_value\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:,0], self.bounds[:,1])\n",
        "                value = self.objective(particle.position)\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "\n",
        "            # Reward: improvement in global best value\n",
        "            reward = old_global_best - self.global_best_value\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            # Update policy network using the REINFORCE rule: maximize reward\n",
        "            loss = -log_prob * reward_tensor\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hcksn8R5xh8",
        "outputId": "18594064-fbee-426c-eea6-5978ad420735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running baseline PSO...\n",
            "\n",
            "Baseline PSO solution:\n",
            "Best position: [-1.49088814e-04 -9.94986795e-01]\n",
            "Best value: 0.9949636240642903\n",
            "\n",
            "Running RL-enhanced PSO (RLPSO)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training progress: 100%|██████████| 30/30 [00:00<00:00, 359.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RLPSO solution:\n",
            "Best position: [-2.00512650e-08 -2.37966252e-06]\n",
            "Best value: 1.1235314900659432e-09\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Testing both the baseline PSO and the RL-enhanced PSO\n",
        "dim = 2\n",
        "bounds = np.array([[-2,2] for _ in range(dim)])\n",
        "num_particles = 40\n",
        "num_iter = 30\n",
        "\n",
        "# generate num_particles number of random sequences of length dim each\n",
        "random_sequences = [np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)]) for _ in range(num_particles)]\n",
        "\n",
        "\n",
        "print(\"Running baseline PSO...\")\n",
        "pso_solver = PSO(dim, bounds, num_particles, num_iter, random_sequences=random_sequences)\n",
        "best_position_pso, best_value_pso, X_list_pso, V_list_pso, pso_results = pso_solver.optimize()\n",
        "print(\"\\nBaseline PSO solution:\")\n",
        "print(\"Best position:\", best_position_pso)\n",
        "print(\"Best value:\", best_value_pso)\n",
        "\n",
        "print(\"\\nRunning RL-enhanced PSO (RLPSO)...\")\n",
        "rl_pso_solver = RLPSO(dim, bounds, num_particles, num_iter, random_sequences=random_sequences)\n",
        "best_position_rl, best_value_rl, X_list, V_list, rl_pso_results = rl_pso_solver.optimize()\n",
        "print(\"\\nRLPSO solution:\")\n",
        "print(\"Best position:\", best_position_rl)\n",
        "print(\"Best value:\", best_value_rl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "1vtQOMmrcvVv",
        "outputId": "2e7be89e-a640-4420-99c8-0ab0831a90d8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# prompt: plot the history from the pso algorithms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the history of the global best value for both PSO algorithms\n",
        "plt.plot(pso_results, label='PSO')\n",
        "plt.plot(rl_pso_results, label='RL-PSO')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Global Best Value')\n",
        "plt.title('Convergence History')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mcMtxh9X1Q_",
        "outputId": "7adc230c-088f-46db-d0ff-18cbe0f03ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-opt\n",
            "  Downloading scikit_opt-0.6.6-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scikit-opt) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from scikit-opt) (1.15.2)\n",
            "Downloading scikit_opt-0.6.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: scikit-opt\n",
            "Successfully installed scikit-opt-0.6.6\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv0HrojN_qQ2"
      },
      "source": [
        "## Training some simple models to test hyperparameter tuning capabilities\n",
        "\n",
        "#### Model 1: Simple MLP for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "biKxefM8_pOO"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, hidden_size=128, dropout_rate=0.2):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tom0MLr4_zlN"
      },
      "outputs": [],
      "source": [
        "def train_mnist(\n",
        "    hidden_size=128,\n",
        "    dropout_rate=0.2,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    epochs=5,\n",
        "    weight_decay=0.0,\n",
        "    momentum=0.9,\n",
        "    optimizer_type='adam'\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a simple MLP on MNIST dataset\n",
        "\n",
        "    Parameters:\n",
        "    - hidden_size: number of neurons in the hidden layer\n",
        "    - dropout_rate: dropout probability\n",
        "    - learning_rate: learning rate for the optimizer\n",
        "    - batch_size: size of mini-batches\n",
        "    - epochs: number of training epochs\n",
        "    - weight_decay: L2 regularization parameter\n",
        "    - momentum: momentum parameter (for SGD)\n",
        "    - optimizer_type: 'adam' or 'sgd'\n",
        "\n",
        "    Returns:\n",
        "    - trained model\n",
        "    - dictionary with training history\n",
        "    \"\"\"\n",
        "    if batch_size < 32:\n",
        "        batch_size = 2**batch_size\n",
        "\n",
        "\n",
        "    # Data loading\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleMLP(hidden_size=hidden_size, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = correct / total\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "082TtNIzAG44"
      },
      "source": [
        "#### Model 2: CNN for CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cZh5ExCN_7yM"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, filters1=32, filters2=64, dropout_rate=0.25):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, filters1, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(filters1)\n",
        "        self.conv2 = nn.Conv2d(filters1, filters2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(filters2)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(filters2 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lwo5_IKfAVC6"
      },
      "outputs": [],
      "source": [
        "def train_cifar10(\n",
        "    filters1=32,\n",
        "    filters2=64,\n",
        "    dropout_rate=0.25,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    epochs=10,\n",
        "    weight_decay=0.0001,\n",
        "    optimizer_type='adam'\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a simple CNN on CIFAR-10 dataset\n",
        "\n",
        "    Parameters:\n",
        "    - filters1: number of filters in first conv layer\n",
        "    - filters2: number of filters in second conv layer\n",
        "    - dropout_rate: dropout probability\n",
        "    - learning_rate: learning rate for the optimizer\n",
        "    - batch_size: size of mini-batches\n",
        "    - epochs: number of training epochs\n",
        "    - weight_decay: L2 regularization parameter\n",
        "    - optimizer_type: 'adam' or 'sgd'\n",
        "\n",
        "    Returns:\n",
        "    - trained model\n",
        "    - dictionary with training history\n",
        "    \"\"\"\n",
        "    if batch_size < 32:\n",
        "        batch_size = 2**batch_size\n",
        "\n",
        "    # Data loading\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model and optimizer\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SimpleCNN(filters1=filters1, filters2=filters2, dropout_rate=dropout_rate).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "\n",
        "    # Training loop\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_acc = correct / total\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(test_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {epoch_loss:.4f}, '\n",
        "              f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQFo-7gAl3s"
      },
      "source": [
        "## Baseline methods for hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b0Ors2tzAahr"
      },
      "outputs": [],
      "source": [
        "def random_search(model_func, param_grid, n_trials=10):\n",
        "    \"\"\"\n",
        "    Perform random search for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible values\n",
        "    - n_trials: number of random combinations to try\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "\n",
        "    for _ in tqdm(range(n_trials)):\n",
        "        # Sample random hyperparameters\n",
        "        params = {k: random.choice(v) if isinstance(v, list) else v for k, v in param_grid.items()}\n",
        "        params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "\n",
        "        # Train model with sampled hyperparameters\n",
        "        _, history = model_func(**params)\n",
        "\n",
        "        # Use the best test accuracy as the score\n",
        "        score = max(history['test_acc'])\n",
        "\n",
        "        results.append((params, score))\n",
        "\n",
        "        # Update best parameters if needed\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "T2mTAKBbAytC"
      },
      "outputs": [],
      "source": [
        "\n",
        "def grid_search(model_func, param_grid):\n",
        "    \"\"\"\n",
        "    Perform grid search for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible values\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate all combinations of parameters\n",
        "    keys = param_grid.keys()\n",
        "\n",
        "    # Make sure all values are iterable (convert single values to lists)\n",
        "    values = []\n",
        "    for key in keys:\n",
        "        if isinstance(param_grid[key], (list, tuple)):\n",
        "            values.append(param_grid[key])\n",
        "        else:\n",
        "            # If the parameter is a single value (like an integer), wrap it in a list\n",
        "            values.append([param_grid[key]])\n",
        "\n",
        "    param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
        "\n",
        "    results = []\n",
        "    best_score = float('-inf')\n",
        "    best_params = None\n",
        "\n",
        "    for params in tqdm(param_combinations):\n",
        "        # Train model with parameters\n",
        "        _, history = model_func(**params)\n",
        "\n",
        "        # Use the best test accuracy as the score\n",
        "        score = max(history['test_acc'])\n",
        "\n",
        "        results.append((params, score))\n",
        "\n",
        "        # Update best parameters if needed\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = params\n",
        "\n",
        "    return best_params, best_score, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "GAYyHRxpqowN"
      },
      "outputs": [],
      "source": [
        "# Particle Swarm Optimization implementation (baseline)\n",
        "class Particle:\n",
        "    def __init__(self, dim, bounds, random_sequence=None):\n",
        "        # replace random with some sequence for experimentation\n",
        "        if random_sequence is not None:\n",
        "            self.position = random_sequence\n",
        "        else:\n",
        "            self.position = np.array([random.uniform(bounds[i][0], bounds[i][1]) for i in range(dim)])\n",
        "        self.velocity = np.zeros(dim)\n",
        "        self.best_position = np.copy(self.position)\n",
        "        # use train_mnist as the objective function\n",
        "        params = {k: v for k, v in zip(param_grid_mnist.keys(), self.position)}\n",
        "        params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "        for k in ['batch_size', 'hidden_size', 'epochs']:\n",
        "            params[k] = int(params[k])\n",
        "        self.best_value = train_mnist(**params)[1]['test_acc'][-1]\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, random_sequences=None):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.num_particles = num_particles\n",
        "        self.num_iter = num_iter\n",
        "        if random_sequences is None:\n",
        "            self.swarm = [Particle(dim, bounds) for _ in range(num_particles)]\n",
        "        else :\n",
        "            self.swarm = [Particle(dim, bounds, random_sequence) for random_sequence in random_sequences]\n",
        "        self.global_best_position = np.copy(self.swarm[0].position)\n",
        "        self.global_best_value = self.swarm[0].best_value\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "        c1, c2 = 1.5, 1.5\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "        for _ in range(self.num_iter):\n",
        "            X = []\n",
        "            V = []\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:, 0], self.bounds[:, 1])\n",
        "                params = {k: v for k, v in zip(param_grid_mnist.keys(), particle.position)}\n",
        "                params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "                params['batch_size'] = int(params['batch_size'])\n",
        "                for k in ['batch_size', 'hidden_size', 'epochs']:\n",
        "                    params[k] = int(params[k])\n",
        "                value = train_mnist(**params)[1]['test_acc'][-1]\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "\n",
        "            print('best value: ', self.global_best_value)\n",
        "\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UA-hwhSjCOSt"
      },
      "outputs": [],
      "source": [
        "def PSO_hyperparam(model_func, param_grid, num_particles=10, num_iter=5, w=0.7, c1=2, c2=2):\n",
        "    \"\"\"\n",
        "    Perform PSO for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_grid: dictionary with hyperparameter names as keys and lists of possible\n",
        "    values\n",
        "    - num_particles: number of particles in the PSO\n",
        "    - num_iter: number of iterations\n",
        "    - w: inertia weight\n",
        "    - c1: cognitive\n",
        "    - c2: social\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    - results: list of (params, score) tuples for all trials\n",
        "    \"\"\"\n",
        "\n",
        "    pso_solver = PSO(dim=len(param_grid), bounds=np.array([[min(v), max(v)] for v in param_grid.values()]), num_particles=num_particles, num_iter=num_iter)\n",
        "    best_position, best_value, X_list, V_list, pso_results = pso_solver.optimize()\n",
        "    best_params = {k: v for k, v in zip(param_grid.keys(), best_position)}\n",
        "    return best_params, best_value, pso_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61QAqUyByPQI"
      },
      "outputs": [],
      "source": [
        "# Policy Network for the RL component\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.action_mean = nn.Linear(hidden_size, action_dim)\n",
        "        # Learnable log_std parameter for the Gaussian distribution\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mean = self.action_mean(x)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return mean, std\n",
        "\n",
        "    def select_action(self, state):\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum()  # Sum log probabilities across dimensions\n",
        "        # Clamp actions to a plausible range for c1 and c2 (e.g., [1.0, 2.0])\n",
        "        action = torch.clamp(action, 1.0, 2.0)\n",
        "        return action, log_prob\n",
        "\n",
        "# RL-enhanced PSO using the policy network to select acceleration coefficients c1 and c2\n",
        "class RLPSO(PSO):\n",
        "    def __init__(self, dim, bounds, num_particles=30, num_iter=100, lr=1e-2, random_sequences=None):\n",
        "        super().__init__(dim, bounds, num_particles, num_iter, objective, random_sequences)\n",
        "        # Define a simple state: [iteration_ratio, global_best_value]\n",
        "        self.state_dim = 2\n",
        "        self.action_dim = 2  # one for c1 and one for c2\n",
        "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.history_X = []\n",
        "        self.history_V = []\n",
        "        self.history_results = []\n",
        "\n",
        "    def get_state(self, iteration):\n",
        "        # Normalize iteration to [0, 1]\n",
        "        iter_norm = iteration / self.num_iter\n",
        "        # Use the current global best value (optionally, further normalization can be applied)\n",
        "        state = np.array([iter_norm, self.global_best_value], dtype=np.float32)\n",
        "        return torch.tensor(state)\n",
        "\n",
        "    def optimize(self):\n",
        "        w = 0.5\n",
        "\n",
        "        X = [particle.position for particle in self.swarm]\n",
        "        V = [particle.velocity for particle in self.swarm]\n",
        "        self.history_X.append(X)\n",
        "        self.history_V.append(V)\n",
        "        self.history_results.append(self.global_best_value)\n",
        "\n",
        "        for it in tqdm(range(1, self.num_iter + 1), desc=\"Training progress\"):\n",
        "            state = self.get_state(it)\n",
        "            action, log_prob = self.policy_net.select_action(state)\n",
        "            c1, c2 = action.detach().numpy()  # use the selected coefficients for this iteration\n",
        "            X = []\n",
        "            V = []\n",
        "            # Store the old global best for reward computation\n",
        "            old_global_best = self.global_best_value\n",
        "            for particle in self.swarm:\n",
        "                r1, r2 = random.random(), random.random()\n",
        "                particle.velocity = (w * particle.velocity +\n",
        "                                     c1 * r1 * (particle.best_position - particle.position) +\n",
        "                                     c2 * r2 * (self.global_best_position - particle.position))\n",
        "                particle.position = np.clip(particle.position + particle.velocity, self.bounds[:,0], self.bounds[:,1])\n",
        "                # use train_mnist as the objective function\n",
        "                params = {k: v for k, v in zip(param_grid_mnist.keys(), particle.position)}\n",
        "                params['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "                for k in ['batch_size', 'hidden_size', 'epochs']:\n",
        "                    params[k] = int(params[k])\n",
        "                value = train_mnist(**params)[1]['test_acc'][-1]\n",
        "\n",
        "                X.append(particle.position)\n",
        "                V.append(particle.velocity)\n",
        "                if value < particle.best_value:\n",
        "                    particle.best_position, particle.best_value = particle.position, value\n",
        "                if value < self.global_best_value:\n",
        "                    self.global_best_position, self.global_best_value = particle.position, value\n",
        "            self.history_X.append(X)\n",
        "            self.history_V.append(V)\n",
        "            self.history_results.append(self.global_best_value)\n",
        "\n",
        "            # Reward: improvement in global best value\n",
        "            reward = old_global_best - self.global_best_value\n",
        "            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n",
        "\n",
        "            # Update policy network using the REINFORCE rule: maximize reward\n",
        "            loss = -log_prob * reward_tensor\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return self.global_best_position, self.global_best_value, self.history_X, self.history_V, self.history_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AT9IWIuKCAdl"
      },
      "outputs": [],
      "source": [
        "def RL_PSO_hyperparam(model_func, param_grid, num_particles=10, num_iter=5):\n",
        "    rl_pso_solver = RLPSO(dim=len(param_grid), bounds=np.array([[min(v), max(v)] for v in param_grid.values()]), num_particles=num_particles, num_iter=num_iter)\n",
        "    best_position, best_value, X_list, V_list, rl_pso_results = rl_pso_solver.optimize()\n",
        "    best_params = {k: v for k, v in zip(param_grid.keys(), best_position)}\n",
        "    return best_params, best_value, rl_pso_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AZr6QBaxYRcj"
      },
      "outputs": [],
      "source": [
        "from skopt import gp_minimize\n",
        "\n",
        "def bayesian_optimization(model_func, param_space, n_calls=10):\n",
        "    \"\"\"\n",
        "    Perform Bayesian Optimization for hyperparameter tuning\n",
        "\n",
        "    Parameters:\n",
        "    - model_func: function that trains the model with hyperparameters\n",
        "    - param_space: list of skopt.space dimensions defining the search space\n",
        "    - n_calls: number of iterations for optimization\n",
        "\n",
        "    Returns:\n",
        "    - best_params: dictionary with best hyperparameters\n",
        "    - best_score: best validation score\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the objective function (minimize negative accuracy)\n",
        "    def objective(params):\n",
        "        param_dict = {dim.name: param for dim, param in zip(param_space, params)}\n",
        "        param_dict['optimizer_type'] = 'adam'  # Fixed optimizer type for simplicity\n",
        "        param_dict['batch_size'] = int(param_dict['batch_size'])\n",
        "        _, history = model_func(**param_dict)\n",
        "        return -max(history['test_acc'])  # Return negative accuracy for minimization\n",
        "\n",
        "    # Run Bayesian optimization\n",
        "    result = gp_minimize(\n",
        "        objective,\n",
        "        param_space,\n",
        "        n_calls=n_calls,\n",
        "        random_state=42,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Extract the best parameters\n",
        "    best_params = {dim.name: result.x[i] for i, dim in enumerate(param_space)}\n",
        "    best_score = -result.fun\n",
        "\n",
        "    return best_params, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Qhp9zi5ZA5q2"
      },
      "outputs": [],
      "source": [
        "param_grid_mnist = {\n",
        "    'hidden_size': [64, 128, 256],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'batch_size': [5, 6, 7],\n",
        "    'epochs': [3,4],  # Fixed for faster execution\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNPWvBA-A9EB",
        "outputId": "f28a65f4-8b1f-4a76-f3c9-bd212e05af34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Example 1: MNIST\n",
            "\n",
            "Random Search\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Train Loss: 0.4122, Test Loss: 0.2490, Test Acc: 0.9328\n",
            "Epoch 2/3, Train Loss: 0.3584, Test Loss: 0.2583, Test Acc: 0.9343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 1/15 [00:17<03:59, 17.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3313, Test Loss: 0.3044, Test Acc: 0.9237\n",
            "Epoch 1/4, Train Loss: 0.4946, Test Loss: 0.2662, Test Acc: 0.9214\n",
            "Epoch 2/4, Train Loss: 0.2562, Test Loss: 0.2060, Test Acc: 0.9403\n",
            "Epoch 3/4, Train Loss: 0.2021, Test Loss: 0.1673, Test Acc: 0.9493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 2/15 [00:38<04:13, 19.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.1676, Test Loss: 0.1431, Test Acc: 0.9565\n",
            "Epoch 1/3, Train Loss: 0.3924, Test Loss: 0.2441, Test Acc: 0.9295\n",
            "Epoch 2/3, Train Loss: 0.3201, Test Loss: 0.2007, Test Acc: 0.9430\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 3/15 [00:52<03:23, 16.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3021, Test Loss: 0.2170, Test Acc: 0.9438\n",
            "Epoch 1/3, Train Loss: 0.3154, Test Loss: 0.1450, Test Acc: 0.9546\n",
            "Epoch 2/3, Train Loss: 0.1682, Test Loss: 0.1188, Test Acc: 0.9629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 4/15 [01:06<02:54, 15.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1334, Test Loss: 0.0983, Test Acc: 0.9701\n",
            "Epoch 1/3, Train Loss: 0.2505, Test Loss: 0.1159, Test Acc: 0.9642\n",
            "Epoch 2/3, Train Loss: 0.1326, Test Loss: 0.0907, Test Acc: 0.9695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 5/15 [01:23<02:43, 16.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1079, Test Loss: 0.0898, Test Acc: 0.9731\n",
            "Epoch 1/3, Train Loss: 0.5634, Test Loss: 0.3309, Test Acc: 0.9182\n",
            "Epoch 2/3, Train Loss: 0.5130, Test Loss: 0.3399, Test Acc: 0.9159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 6/15 [01:39<02:25, 16.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.4977, Test Loss: 0.3509, Test Acc: 0.9283\n",
            "Epoch 1/3, Train Loss: 0.4421, Test Loss: 0.2604, Test Acc: 0.9311\n",
            "Epoch 2/3, Train Loss: 0.4040, Test Loss: 0.2376, Test Acc: 0.9368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 7/15 [01:54<02:06, 15.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3717, Test Loss: 0.2338, Test Acc: 0.9410\n",
            "Epoch 1/4, Train Loss: 0.3116, Test Loss: 0.1899, Test Acc: 0.9434\n",
            "Epoch 2/4, Train Loss: 0.2183, Test Loss: 0.1647, Test Acc: 0.9519\n",
            "Epoch 3/4, Train Loss: 0.1989, Test Loss: 0.1647, Test Acc: 0.9542\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 8/15 [02:12<01:54, 16.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.1902, Test Loss: 0.1784, Test Acc: 0.9492\n",
            "Epoch 1/4, Train Loss: 0.4418, Test Loss: 0.1961, Test Acc: 0.9402\n",
            "Epoch 2/4, Train Loss: 0.2430, Test Loss: 0.1494, Test Acc: 0.9556\n",
            "Epoch 3/4, Train Loss: 0.2038, Test Loss: 0.1310, Test Acc: 0.9604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 9/15 [02:29<01:40, 16.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.1784, Test Loss: 0.1193, Test Acc: 0.9635\n",
            "Epoch 1/4, Train Loss: 0.2877, Test Loss: 0.1411, Test Acc: 0.9581\n",
            "Epoch 2/4, Train Loss: 0.1506, Test Loss: 0.1059, Test Acc: 0.9689\n",
            "Epoch 3/4, Train Loss: 0.1197, Test Loss: 0.1005, Test Acc: 0.9689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 10/15 [02:51<01:31, 18.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.1028, Test Loss: 0.0979, Test Acc: 0.9708\n",
            "Epoch 1/4, Train Loss: 0.2523, Test Loss: 0.1337, Test Acc: 0.9578\n",
            "Epoch 2/4, Train Loss: 0.1361, Test Loss: 0.0883, Test Acc: 0.9722\n",
            "Epoch 3/4, Train Loss: 0.1117, Test Loss: 0.0892, Test Acc: 0.9718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 11/15 [03:14<01:18, 19.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.0950, Test Loss: 0.0742, Test Acc: 0.9770\n",
            "Epoch 1/3, Train Loss: 0.3703, Test Loss: 0.2145, Test Acc: 0.9382\n",
            "Epoch 2/3, Train Loss: 0.2751, Test Loss: 0.1962, Test Acc: 0.9415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 12/15 [03:27<00:53, 17.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2618, Test Loss: 0.1809, Test Acc: 0.9491\n",
            "Epoch 1/3, Train Loss: 0.2858, Test Loss: 0.1501, Test Acc: 0.9557\n",
            "Epoch 2/3, Train Loss: 0.1970, Test Loss: 0.1743, Test Acc: 0.9511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 13/15 [03:40<00:32, 16.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1874, Test Loss: 0.1898, Test Acc: 0.9513\n",
            "Epoch 1/3, Train Loss: 0.9347, Test Loss: 0.4375, Test Acc: 0.8914\n",
            "Epoch 2/3, Train Loss: 0.4150, Test Loss: 0.3222, Test Acc: 0.9123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 14/15 [03:54<00:15, 15.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3398, Test Loss: 0.2780, Test Acc: 0.9213\n",
            "Epoch 1/4, Train Loss: 0.6345, Test Loss: 0.3127, Test Acc: 0.9128\n",
            "Epoch 2/4, Train Loss: 0.3090, Test Loss: 0.2377, Test Acc: 0.9322\n",
            "Epoch 3/4, Train Loss: 0.2478, Test Loss: 0.2012, Test Acc: 0.9427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [04:13<00:00, 16.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/4, Train Loss: 0.2123, Test Loss: 0.1754, Test Acc: 0.9485\n",
            "Best accuracy: 0.9770\n",
            "Best parameters: {'hidden_size': 256, 'dropout_rate': 0.3, 'learning_rate': 0.001, 'batch_size': 5, 'epochs': 4, 'optimizer_type': 'adam'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nExample 1: MNIST\")\n",
        "print(\"\\nRandom Search\")\n",
        "best_params, best_score, results = random_search(train_mnist, param_grid_mnist, n_trials=15)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FVn3UHHmBE4F",
        "outputId": "bb28e1d4-52c3-4038-b066-4835105eb5cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Grid Search\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/81 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Train Loss: 0.5999, Test Loss: 0.3087, Test Acc: 0.9169\n",
            "Epoch 2/3, Train Loss: 0.3105, Test Loss: 0.2491, Test Acc: 0.9296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 1/81 [00:50<1:07:04, 50.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2574, Test Loss: 0.2095, Test Acc: 0.9396\n",
            "Epoch 1/3, Train Loss: 0.7384, Test Loss: 0.3496, Test Acc: 0.9070\n",
            "Epoch 2/3, Train Loss: 0.3526, Test Loss: 0.2781, Test Acc: 0.9192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 2/81 [01:34<1:01:33, 46.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2934, Test Loss: 0.2399, Test Acc: 0.9306\n",
            "Epoch 1/3, Train Loss: 0.9114, Test Loss: 0.4315, Test Acc: 0.8924\n",
            "Epoch 2/3, Train Loss: 0.4152, Test Loss: 0.3234, Test Acc: 0.9101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▎         | 3/81 [02:16<57:54, 44.55s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3407, Test Loss: 0.2811, Test Acc: 0.9208\n",
            "Epoch 1/3, Train Loss: 0.2884, Test Loss: 0.1539, Test Acc: 0.9526\n",
            "Epoch 2/3, Train Loss: 0.1498, Test Loss: 0.1198, Test Acc: 0.9652\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 4/81 [03:06<59:43, 46.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1205, Test Loss: 0.0931, Test Acc: 0.9709\n",
            "Epoch 1/3, Train Loss: 0.3228, Test Loss: 0.1583, Test Acc: 0.9525\n",
            "Epoch 2/3, Train Loss: 0.1649, Test Loss: 0.1210, Test Acc: 0.9634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 5/81 [03:50<58:08, 45.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1276, Test Loss: 0.1059, Test Acc: 0.9678\n",
            "Epoch 1/3, Train Loss: 0.3619, Test Loss: 0.1919, Test Acc: 0.9448\n",
            "Epoch 2/3, Train Loss: 0.1862, Test Loss: 0.1352, Test Acc: 0.9602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 6/81 [04:32<55:42, 44.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1432, Test Loss: 0.1120, Test Acc: 0.9663\n",
            "Epoch 1/3, Train Loss: 0.4012, Test Loss: 0.2482, Test Acc: 0.9265\n",
            "Epoch 2/3, Train Loss: 0.3502, Test Loss: 0.2742, Test Acc: 0.9309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▊         | 7/81 [05:23<57:31, 46.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3423, Test Loss: 0.3135, Test Acc: 0.9268\n",
            "Epoch 1/3, Train Loss: 0.3512, Test Loss: 0.2323, Test Acc: 0.9379\n",
            "Epoch 2/3, Train Loss: 0.2693, Test Loss: 0.2088, Test Acc: 0.9421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|▉         | 8/81 [06:10<56:38, 46.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2580, Test Loss: 0.2035, Test Acc: 0.9425\n",
            "Epoch 1/3, Train Loss: 0.3090, Test Loss: 0.1794, Test Acc: 0.9428\n",
            "Epoch 2/3, Train Loss: 0.2210, Test Loss: 0.1671, Test Acc: 0.9482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 9/81 [07:09<1:00:46, 50.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1996, Test Loss: 0.1528, Test Acc: 0.9594\n",
            "Epoch 1/3, Train Loss: 0.6413, Test Loss: 0.3140, Test Acc: 0.9112\n",
            "Epoch 2/3, Train Loss: 0.3397, Test Loss: 0.2533, Test Acc: 0.9258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 10/81 [08:11<1:04:00, 54.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2826, Test Loss: 0.2199, Test Acc: 0.9360\n",
            "Epoch 1/3, Train Loss: 0.7821, Test Loss: 0.3631, Test Acc: 0.9053\n",
            "Epoch 2/3, Train Loss: 0.3808, Test Loss: 0.2807, Test Acc: 0.9214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▎        | 11/81 [08:58<1:00:26, 51.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3174, Test Loss: 0.2423, Test Acc: 0.9308\n",
            "Epoch 1/3, Train Loss: 0.9901, Test Loss: 0.4513, Test Acc: 0.8905\n",
            "Epoch 2/3, Train Loss: 0.4511, Test Loss: 0.3296, Test Acc: 0.9111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▍        | 12/81 [09:40<56:25, 49.06s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3645, Test Loss: 0.2826, Test Acc: 0.9185\n",
            "Epoch 1/3, Train Loss: 0.3108, Test Loss: 0.1468, Test Acc: 0.9550\n",
            "Epoch 2/3, Train Loss: 0.1737, Test Loss: 0.1152, Test Acc: 0.9654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 13/81 [10:31<56:11, 49.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1453, Test Loss: 0.1024, Test Acc: 0.9712\n",
            "Epoch 1/3, Train Loss: 0.3560, Test Loss: 0.1781, Test Acc: 0.9472\n",
            "Epoch 2/3, Train Loss: 0.1933, Test Loss: 0.1272, Test Acc: 0.9617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 14/81 [11:18<54:20, 48.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1582, Test Loss: 0.1091, Test Acc: 0.9664\n",
            "Epoch 1/3, Train Loss: 0.4196, Test Loss: 0.2036, Test Acc: 0.9406\n",
            "Epoch 2/3, Train Loss: 0.2172, Test Loss: 0.1453, Test Acc: 0.9568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▊        | 15/81 [12:02<52:09, 47.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1709, Test Loss: 0.1216, Test Acc: 0.9645\n",
            "Epoch 1/3, Train Loss: 0.4835, Test Loss: 0.2963, Test Acc: 0.9223\n",
            "Epoch 2/3, Train Loss: 0.4290, Test Loss: 0.2957, Test Acc: 0.9213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|█▉        | 16/81 [12:54<52:41, 48.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.4177, Test Loss: 0.3025, Test Acc: 0.9188\n",
            "Epoch 1/3, Train Loss: 0.4089, Test Loss: 0.2051, Test Acc: 0.9390\n",
            "Epoch 2/3, Train Loss: 0.3322, Test Loss: 0.1987, Test Acc: 0.9439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██        | 17/81 [13:40<51:01, 47.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3099, Test Loss: 0.2024, Test Acc: 0.9477\n",
            "Epoch 1/3, Train Loss: 0.3766, Test Loss: 0.1923, Test Acc: 0.9429\n",
            "Epoch 2/3, Train Loss: 0.2774, Test Loss: 0.1967, Test Acc: 0.9419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 18/81 [14:22<48:33, 46.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2500, Test Loss: 0.1723, Test Acc: 0.9493\n",
            "Epoch 1/3, Train Loss: 0.6881, Test Loss: 0.3196, Test Acc: 0.9117\n",
            "Epoch 2/3, Train Loss: 0.3620, Test Loss: 0.2524, Test Acc: 0.9259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 19/81 [15:13<49:02, 47.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3034, Test Loss: 0.2179, Test Acc: 0.9360\n",
            "Epoch 1/3, Train Loss: 0.8480, Test Loss: 0.3769, Test Acc: 0.9008\n",
            "Epoch 2/3, Train Loss: 0.4206, Test Loss: 0.2922, Test Acc: 0.9190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▍       | 20/81 [15:58<47:35, 46.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3509, Test Loss: 0.2500, Test Acc: 0.9278\n",
            "Epoch 1/3, Train Loss: 1.0201, Test Loss: 0.4615, Test Acc: 0.8861\n",
            "Epoch 2/3, Train Loss: 0.4928, Test Loss: 0.3407, Test Acc: 0.9068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 21/81 [16:41<45:43, 45.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.4020, Test Loss: 0.2946, Test Acc: 0.9162\n",
            "Epoch 1/3, Train Loss: 0.3642, Test Loss: 0.1620, Test Acc: 0.9484\n",
            "Epoch 2/3, Train Loss: 0.2229, Test Loss: 0.1247, Test Acc: 0.9620\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 27%|██▋       | 22/81 [17:31<46:18, 47.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1949, Test Loss: 0.1160, Test Acc: 0.9639\n",
            "Epoch 1/3, Train Loss: 0.3842, Test Loss: 0.1743, Test Acc: 0.9478\n",
            "Epoch 2/3, Train Loss: 0.2208, Test Loss: 0.1310, Test Acc: 0.9585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 28%|██▊       | 23/81 [18:16<44:56, 46.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1833, Test Loss: 0.1139, Test Acc: 0.9665\n",
            "Epoch 1/3, Train Loss: 0.4493, Test Loss: 0.2031, Test Acc: 0.9390\n",
            "Epoch 2/3, Train Loss: 0.2467, Test Loss: 0.1470, Test Acc: 0.9558\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|██▉       | 24/81 [18:58<42:52, 45.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2077, Test Loss: 0.1307, Test Acc: 0.9609\n",
            "Epoch 1/3, Train Loss: 0.5846, Test Loss: 0.2707, Test Acc: 0.9236\n",
            "Epoch 2/3, Train Loss: 0.5169, Test Loss: 0.3371, Test Acc: 0.9125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 25/81 [19:49<43:37, 46.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.5096, Test Loss: 0.2830, Test Acc: 0.9267\n",
            "Epoch 1/3, Train Loss: 0.4794, Test Loss: 0.2190, Test Acc: 0.9386\n",
            "Epoch 2/3, Train Loss: 0.4035, Test Loss: 0.2334, Test Acc: 0.9407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 26/81 [20:34<42:24, 46.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3791, Test Loss: 0.2273, Test Acc: 0.9406\n",
            "Epoch 1/3, Train Loss: 0.4590, Test Loss: 0.2198, Test Acc: 0.9325\n",
            "Epoch 2/3, Train Loss: 0.3491, Test Loss: 0.2021, Test Acc: 0.9372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 27/81 [21:16<40:33, 45.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3277, Test Loss: 0.1854, Test Acc: 0.9467\n",
            "Epoch 1/3, Train Loss: 0.5022, Test Loss: 0.2727, Test Acc: 0.9238\n",
            "Epoch 2/3, Train Loss: 0.2597, Test Loss: 0.2077, Test Acc: 0.9399\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▍      | 28/81 [22:08<41:31, 47.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2027, Test Loss: 0.1677, Test Acc: 0.9507\n",
            "Epoch 1/3, Train Loss: 0.6256, Test Loss: 0.3125, Test Acc: 0.9138\n",
            "Epoch 2/3, Train Loss: 0.3024, Test Loss: 0.2448, Test Acc: 0.9293\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 29/81 [22:53<40:07, 46.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2477, Test Loss: 0.2064, Test Acc: 0.9396\n",
            "Epoch 1/3, Train Loss: 0.7902, Test Loss: 0.3674, Test Acc: 0.9021\n",
            "Epoch 2/3, Train Loss: 0.3490, Test Loss: 0.2803, Test Acc: 0.9208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 30/81 [23:35<38:28, 45.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2847, Test Loss: 0.2396, Test Acc: 0.9318\n",
            "Epoch 1/3, Train Loss: 0.2467, Test Loss: 0.1299, Test Acc: 0.9596\n",
            "Epoch 2/3, Train Loss: 0.1195, Test Loss: 0.0921, Test Acc: 0.9710\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 31/81 [24:26<39:00, 46.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.0905, Test Loss: 0.0970, Test Acc: 0.9715\n",
            "Epoch 1/3, Train Loss: 0.2767, Test Loss: 0.1337, Test Acc: 0.9595\n",
            "Epoch 2/3, Train Loss: 0.1280, Test Loss: 0.1024, Test Acc: 0.9699\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|███▉      | 32/81 [25:11<37:47, 46.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.0951, Test Loss: 0.0909, Test Acc: 0.9715\n",
            "Epoch 1/3, Train Loss: 0.3210, Test Loss: 0.1629, Test Acc: 0.9522\n",
            "Epoch 2/3, Train Loss: 0.1472, Test Loss: 0.1110, Test Acc: 0.9670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 41%|████      | 33/81 [25:53<36:05, 45.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1077, Test Loss: 0.0963, Test Acc: 0.9701\n",
            "Epoch 1/3, Train Loss: 0.4019, Test Loss: 0.2444, Test Acc: 0.9307\n",
            "Epoch 2/3, Train Loss: 0.3479, Test Loss: 0.3207, Test Acc: 0.9198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 34/81 [26:45<37:00, 47.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.3347, Test Loss: 0.3054, Test Acc: 0.9342\n",
            "Epoch 1/3, Train Loss: 0.3376, Test Loss: 0.1990, Test Acc: 0.9434\n",
            "Epoch 2/3, Train Loss: 0.2529, Test Loss: 0.2313, Test Acc: 0.9368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 35/81 [27:30<35:32, 46.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2500, Test Loss: 0.2214, Test Acc: 0.9498\n",
            "Epoch 1/3, Train Loss: 0.2905, Test Loss: 0.1935, Test Acc: 0.9376\n",
            "Epoch 2/3, Train Loss: 0.1926, Test Loss: 0.1928, Test Acc: 0.9463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 36/81 [28:12<33:51, 45.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.1787, Test Loss: 0.1861, Test Acc: 0.9554\n",
            "Epoch 1/3, Train Loss: 0.5266, Test Loss: 0.2755, Test Acc: 0.9230\n",
            "Epoch 2/3, Train Loss: 0.2766, Test Loss: 0.2146, Test Acc: 0.9377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 37/81 [29:02<34:11, 46.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2196, Test Loss: 0.1719, Test Acc: 0.9510\n",
            "Epoch 1/3, Train Loss: 0.6473, Test Loss: 0.3106, Test Acc: 0.9159\n",
            "Epoch 2/3, Train Loss: 0.3121, Test Loss: 0.2454, Test Acc: 0.9287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 38/81 [29:47<33:06, 46.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2561, Test Loss: 0.2066, Test Acc: 0.9382\n",
            "Epoch 1/3, Train Loss: 0.7911, Test Loss: 0.3675, Test Acc: 0.9032\n",
            "Epoch 2/3, Train Loss: 0.3628, Test Loss: 0.2821, Test Acc: 0.9199\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 39/81 [30:29<31:20, 44.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.2974, Test Loss: 0.2413, Test Acc: 0.9314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 39/81 [30:42<33:03, 47.24s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2331fbae6175>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Grid Search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best accuracy: {best_score:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters: {best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c84f0cb8dfc6>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(model_func, param_grid)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_combinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Train model with parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Use the best test accuracy as the score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-27e88aa407e2>\u001b[0m in \u001b[0;36mtrain_mnist\u001b[0;34m(hidden_size, dropout_rate, learning_rate, batch_size, epochs, weight_decay, momentum, optimizer_type)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"std evaluated to zero after conversion to {dtype}, leading to division by zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"\\n Grid Search\")\n",
        "best_params, best_score, results = grid_search(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_grTOLQIBIBV",
        "outputId": "ba41a40f-6701-410b-c686-3b7e44939412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bayesian Optimization\n",
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.2338, Test Loss: 0.1300, Test Acc: 0.9604\n",
            "Epoch 2/3, Train Loss: 0.1470, Test Loss: 0.1264, Test Acc: 0.9641\n",
            "Epoch 3/3, Train Loss: 0.1261, Test Loss: 0.1035, Test Acc: 0.9700\n",
            "Iteration No: 1 ended. Evaluation done at random point.\n",
            "Time taken: 49.3465\n",
            "Function value obtained: -0.9700\n",
            "Current minimum: -0.9700\n",
            "Iteration No: 2 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.3520, Test Loss: 0.1728, Test Acc: 0.9473\n",
            "Epoch 2/4, Train Loss: 0.1849, Test Loss: 0.1276, Test Acc: 0.9613\n",
            "Epoch 3/4, Train Loss: 0.1455, Test Loss: 0.1049, Test Acc: 0.9668\n",
            "Epoch 4/4, Train Loss: 0.1234, Test Loss: 0.0963, Test Acc: 0.9709\n",
            "Iteration No: 2 ended. Evaluation done at random point.\n",
            "Time taken: 71.4554\n",
            "Function value obtained: -0.9709\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 3 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.4466, Test Loss: 0.2436, Test Acc: 0.9297\n",
            "Epoch 2/4, Train Loss: 0.3824, Test Loss: 0.2134, Test Acc: 0.9415\n",
            "Epoch 3/4, Train Loss: 0.3718, Test Loss: 0.2503, Test Acc: 0.9337\n",
            "Epoch 4/4, Train Loss: 0.3576, Test Loss: 0.2506, Test Acc: 0.9402\n",
            "Iteration No: 3 ended. Evaluation done at random point.\n",
            "Time taken: 71.6607\n",
            "Function value obtained: -0.9415\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 4 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.4831, Test Loss: 0.2562, Test Acc: 0.9262\n",
            "Epoch 2/4, Train Loss: 0.2508, Test Loss: 0.1937, Test Acc: 0.9436\n",
            "Epoch 3/4, Train Loss: 0.1937, Test Loss: 0.1569, Test Acc: 0.9544\n",
            "Epoch 4/4, Train Loss: 0.1593, Test Loss: 0.1302, Test Acc: 0.9619\n",
            "Iteration No: 4 ended. Evaluation done at random point.\n",
            "Time taken: 71.1054\n",
            "Function value obtained: -0.9619\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 5 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.3840, Test Loss: 0.2470, Test Acc: 0.9272\n",
            "Epoch 2/3, Train Loss: 0.3253, Test Loss: 0.2400, Test Acc: 0.9398\n",
            "Epoch 3/3, Train Loss: 0.3111, Test Loss: 0.2525, Test Acc: 0.9391\n",
            "Iteration No: 5 ended. Evaluation done at random point.\n",
            "Time taken: 53.1378\n",
            "Function value obtained: -0.9398\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 6 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.3511, Test Loss: 0.2108, Test Acc: 0.9378\n",
            "Epoch 2/4, Train Loss: 0.2875, Test Loss: 0.2749, Test Acc: 0.9183\n",
            "Epoch 3/4, Train Loss: 0.2727, Test Loss: 0.2064, Test Acc: 0.9489\n",
            "Epoch 4/4, Train Loss: 0.2545, Test Loss: 0.2224, Test Acc: 0.9538\n",
            "Iteration No: 6 ended. Evaluation done at random point.\n",
            "Time taken: 63.8745\n",
            "Function value obtained: -0.9538\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 7 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.6991, Test Loss: 0.3319, Test Acc: 0.9076\n",
            "Epoch 2/4, Train Loss: 0.3268, Test Loss: 0.2591, Test Acc: 0.9271\n",
            "Epoch 3/4, Train Loss: 0.2657, Test Loss: 0.2205, Test Acc: 0.9365\n",
            "Epoch 4/4, Train Loss: 0.2275, Test Loss: 0.1896, Test Acc: 0.9442\n",
            "Iteration No: 7 ended. Evaluation done at random point.\n",
            "Time taken: 58.2896\n",
            "Function value obtained: -0.9442\n",
            "Current minimum: -0.9709\n",
            "Iteration No: 8 started. Evaluating function at random point.\n",
            "Epoch 1/4, Train Loss: 0.3387, Test Loss: 0.1826, Test Acc: 0.9461\n",
            "Epoch 2/4, Train Loss: 0.1658, Test Loss: 0.1241, Test Acc: 0.9639\n",
            "Epoch 3/4, Train Loss: 0.1189, Test Loss: 0.1006, Test Acc: 0.9695\n",
            "Epoch 4/4, Train Loss: 0.0936, Test Loss: 0.0874, Test Acc: 0.9728\n",
            "Iteration No: 8 ended. Evaluation done at random point.\n",
            "Time taken: 69.0991\n",
            "Function value obtained: -0.9728\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 9 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.4451, Test Loss: 0.2297, Test Acc: 0.9317\n",
            "Epoch 2/3, Train Loss: 0.2224, Test Loss: 0.1643, Test Acc: 0.9513\n",
            "Epoch 3/3, Train Loss: 0.1708, Test Loss: 0.1314, Test Acc: 0.9607\n",
            "Iteration No: 9 ended. Evaluation done at random point.\n",
            "Time taken: 46.7251\n",
            "Function value obtained: -0.9607\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 10 started. Evaluating function at random point.\n",
            "Epoch 1/3, Train Loss: 0.3928, Test Loss: 0.2064, Test Acc: 0.9417\n",
            "Epoch 2/3, Train Loss: 0.1903, Test Loss: 0.1454, Test Acc: 0.9578\n",
            "Epoch 3/3, Train Loss: 0.1388, Test Loss: 0.1106, Test Acc: 0.9679\n",
            "Iteration No: 10 ended. Evaluation done at random point.\n",
            "Time taken: 47.1540\n",
            "Function value obtained: -0.9679\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 11 started. Searching for the next optimal point.\n",
            "Epoch 1/3, Train Loss: 0.2402, Test Loss: 0.1242, Test Acc: 0.9625\n",
            "Epoch 2/3, Train Loss: 0.1102, Test Loss: 0.1058, Test Acc: 0.9674\n",
            "Epoch 3/3, Train Loss: 0.0825, Test Loss: 0.0913, Test Acc: 0.9718\n",
            "Iteration No: 11 ended. Search finished for the next optimal point.\n",
            "Time taken: 53.7304\n",
            "Function value obtained: -0.9718\n",
            "Current minimum: -0.9728\n",
            "Iteration No: 12 started. Searching for the next optimal point.\n",
            "Epoch 1/4, Train Loss: 0.2631, Test Loss: 0.1304, Test Acc: 0.9599\n",
            "Epoch 2/4, Train Loss: 0.1238, Test Loss: 0.0946, Test Acc: 0.9696\n",
            "Epoch 3/4, Train Loss: 0.0949, Test Loss: 0.0924, Test Acc: 0.9717\n",
            "Epoch 4/4, Train Loss: 0.0780, Test Loss: 0.0795, Test Acc: 0.9752\n",
            "Iteration No: 12 ended. Search finished for the next optimal point.\n",
            "Time taken: 58.7340\n",
            "Function value obtained: -0.9752\n",
            "Current minimum: -0.9752\n",
            "Iteration No: 13 started. Searching for the next optimal point.\n",
            "Epoch 1/3, Train Loss: 0.2922, Test Loss: 0.1383, Test Acc: 0.9598\n",
            "Epoch 2/3, Train Loss: 0.1272, Test Loss: 0.1031, Test Acc: 0.9674\n",
            "Epoch 3/3, Train Loss: 0.0890, Test Loss: 0.0785, Test Acc: 0.9758\n",
            "Iteration No: 13 ended. Search finished for the next optimal point.\n",
            "Time taken: 44.0198\n",
            "Function value obtained: -0.9758\n",
            "Current minimum: -0.9758\n",
            "Iteration No: 14 started. Searching for the next optimal point.\n",
            "Epoch 1/3, Train Loss: 0.2706, Test Loss: 0.1453, Test Acc: 0.9528\n",
            "Epoch 2/3, Train Loss: 0.1832, Test Loss: 0.1143, Test Acc: 0.9648\n",
            "Epoch 3/3, Train Loss: 0.1601, Test Loss: 0.1237, Test Acc: 0.9660\n",
            "Iteration No: 14 ended. Search finished for the next optimal point.\n",
            "Time taken: 42.9045\n",
            "Function value obtained: -0.9660\n",
            "Current minimum: -0.9758\n",
            "Iteration No: 15 started. Searching for the next optimal point.\n",
            "Epoch 1/4, Train Loss: 0.2250, Test Loss: 0.1134, Test Acc: 0.9658\n",
            "Epoch 2/4, Train Loss: 0.1240, Test Loss: 0.1019, Test Acc: 0.9691\n",
            "Epoch 3/4, Train Loss: 0.1028, Test Loss: 0.0898, Test Acc: 0.9747\n",
            "Epoch 4/4, Train Loss: 0.0891, Test Loss: 0.1027, Test Acc: 0.9724\n",
            "Iteration No: 15 ended. Search finished for the next optimal point.\n",
            "Time taken: 68.4992\n",
            "Function value obtained: -0.9747\n",
            "Current minimum: -0.9758\n",
            "Best accuracy: 0.9758\n",
            "Best parameters: {'hidden_size': np.int64(245), 'dropout_rate': 0.10906815506053369, 'learning_rate': 0.0008393492851411112, 'batch_size': np.int64(7), 'epochs': np.int64(3)}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nBayesian Optimization\")\n",
        "\n",
        "from skopt.space import Integer, Real\n",
        "\n",
        "best_params, best_score = bayesian_optimization(\n",
        "    train_mnist,\n",
        "    [\n",
        "        Integer(64, 256, name='hidden_size'),\n",
        "        Real(0.1, 0.3, name='dropout_rate'),\n",
        "        Real(1e-4, 1e-2, prior='log-uniform', name='learning_rate'),\n",
        "        Integer(5, 7, name='batch_size'), # in powers of 2\n",
        "        Integer(3, 4 , name='epochs')\n",
        "    ],\n",
        "    n_calls=15\n",
        ")\n",
        "\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv8ez-bbB_Y0",
        "outputId": "a3ec30a7-cd7f-4c1d-b382-a9c1c80d4a5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " PSO\n",
            "Epoch 1/3, Train Loss: 0.2887, Test Loss: 0.1447, Test Acc: 0.9553\n",
            "Epoch 2/3, Train Loss: 0.2017, Test Loss: 0.1269, Test Acc: 0.9616\n",
            "Epoch 3/3, Train Loss: 0.1816, Test Loss: 0.1125, Test Acc: 0.9694\n",
            "Epoch 1/3, Train Loss: 0.2658, Test Loss: 0.1439, Test Acc: 0.9581\n",
            "Epoch 2/3, Train Loss: 0.1637, Test Loss: 0.1276, Test Acc: 0.9626\n",
            "Epoch 3/3, Train Loss: 0.1479, Test Loss: 0.1301, Test Acc: 0.9623\n",
            "Epoch 1/3, Train Loss: 0.2375, Test Loss: 0.1228, Test Acc: 0.9606\n",
            "Epoch 2/3, Train Loss: 0.1553, Test Loss: 0.1380, Test Acc: 0.9602\n",
            "Epoch 3/3, Train Loss: 0.1330, Test Loss: 0.1476, Test Acc: 0.9599\n",
            "Epoch 1/3, Train Loss: 0.4942, Test Loss: 0.2987, Test Acc: 0.9114\n",
            "Epoch 2/3, Train Loss: 0.4337, Test Loss: 0.2846, Test Acc: 0.9306\n",
            "Epoch 3/3, Train Loss: 0.4167, Test Loss: 0.2851, Test Acc: 0.9253\n",
            "Epoch 1/3, Train Loss: 0.6356, Test Loss: 0.3032, Test Acc: 0.9153\n",
            "Epoch 2/3, Train Loss: 0.3313, Test Loss: 0.2370, Test Acc: 0.9311\n",
            "Epoch 3/3, Train Loss: 0.2712, Test Loss: 0.2009, Test Acc: 0.9404\n",
            "Epoch 1/3, Train Loss: 0.4234, Test Loss: 0.2481, Test Acc: 0.9367\n",
            "Epoch 2/3, Train Loss: 0.3536, Test Loss: 0.3022, Test Acc: 0.9154\n",
            "Epoch 3/3, Train Loss: 0.3453, Test Loss: 0.3238, Test Acc: 0.9291\n",
            "Epoch 1/3, Train Loss: 0.2958, Test Loss: 0.1645, Test Acc: 0.9510\n",
            "Epoch 2/3, Train Loss: 0.2244, Test Loss: 0.1799, Test Acc: 0.9539\n",
            "Epoch 3/3, Train Loss: 0.2104, Test Loss: 0.1803, Test Acc: 0.9541\n",
            "Epoch 1/3, Train Loss: 0.2671, Test Loss: 0.1675, Test Acc: 0.9524\n",
            "Epoch 2/3, Train Loss: 0.1847, Test Loss: 0.1359, Test Acc: 0.9636\n",
            "Epoch 3/3, Train Loss: 0.1805, Test Loss: 0.1557, Test Acc: 0.9580\n",
            "Epoch 1/3, Train Loss: 0.2871, Test Loss: 0.1581, Test Acc: 0.9513\n",
            "Epoch 2/3, Train Loss: 0.2092, Test Loss: 0.1527, Test Acc: 0.9541\n",
            "Epoch 3/3, Train Loss: 0.1932, Test Loss: 0.1780, Test Acc: 0.9512\n",
            "Epoch 1/3, Train Loss: 0.3416, Test Loss: 0.1727, Test Acc: 0.9505\n",
            "Epoch 2/3, Train Loss: 0.1568, Test Loss: 0.1184, Test Acc: 0.9635\n",
            "Epoch 3/3, Train Loss: 0.1111, Test Loss: 0.0931, Test Acc: 0.9717\n",
            "Epoch 1/3, Train Loss: 0.2972, Test Loss: 0.1389, Test Acc: 0.9566\n",
            "Epoch 2/3, Train Loss: 0.2050, Test Loss: 0.1171, Test Acc: 0.9676\n",
            "Epoch 3/3, Train Loss: 0.1819, Test Loss: 0.1333, Test Acc: 0.9647\n",
            "Epoch 1/3, Train Loss: 0.2768, Test Loss: 0.1397, Test Acc: 0.9569\n",
            "Epoch 2/3, Train Loss: 0.1854, Test Loss: 0.1377, Test Acc: 0.9603\n",
            "Epoch 3/3, Train Loss: 0.1692, Test Loss: 0.1138, Test Acc: 0.9669\n",
            "Epoch 1/3, Train Loss: 0.2400, Test Loss: 0.1454, Test Acc: 0.9579\n",
            "Epoch 2/3, Train Loss: 0.1505, Test Loss: 0.1165, Test Acc: 0.9692\n",
            "Epoch 3/3, Train Loss: 0.1305, Test Loss: 0.1168, Test Acc: 0.9681\n",
            "Epoch 1/3, Train Loss: 0.3986, Test Loss: 0.2037, Test Acc: 0.9365\n",
            "Epoch 2/3, Train Loss: 0.3167, Test Loss: 0.1894, Test Acc: 0.9438\n",
            "Epoch 3/3, Train Loss: 0.3014, Test Loss: 0.1830, Test Acc: 0.9508\n",
            "Epoch 1/3, Train Loss: 0.3349, Test Loss: 0.1875, Test Acc: 0.9473\n",
            "Epoch 2/3, Train Loss: 0.2559, Test Loss: 0.1589, Test Acc: 0.9549\n",
            "Epoch 3/3, Train Loss: 0.2332, Test Loss: 0.1530, Test Acc: 0.9597\n",
            "Epoch 1/3, Train Loss: 0.4342, Test Loss: 0.2965, Test Acc: 0.9301\n",
            "Epoch 2/3, Train Loss: 0.3623, Test Loss: 0.2497, Test Acc: 0.9379\n",
            "Epoch 3/3, Train Loss: 0.3358, Test Loss: 0.2474, Test Acc: 0.9475\n",
            "Epoch 1/3, Train Loss: 0.2992, Test Loss: 0.1862, Test Acc: 0.9436\n",
            "Epoch 2/3, Train Loss: 0.2244, Test Loss: 0.1635, Test Acc: 0.9553\n",
            "Epoch 3/3, Train Loss: 0.2143, Test Loss: 0.1662, Test Acc: 0.9572\n",
            "Epoch 1/3, Train Loss: 0.3656, Test Loss: 0.2373, Test Acc: 0.9340\n",
            "Epoch 2/3, Train Loss: 0.2935, Test Loss: 0.2372, Test Acc: 0.9438\n",
            "Epoch 3/3, Train Loss: 0.2871, Test Loss: 0.2225, Test Acc: 0.9468\n",
            "Epoch 1/3, Train Loss: 0.3752, Test Loss: 0.2355, Test Acc: 0.9373\n",
            "Epoch 2/3, Train Loss: 0.3016, Test Loss: 0.2388, Test Acc: 0.9394\n",
            "Epoch 3/3, Train Loss: 0.2924, Test Loss: 0.2320, Test Acc: 0.9400\n",
            "Epoch 1/3, Train Loss: 0.2519, Test Loss: 0.1178, Test Acc: 0.9619\n",
            "Epoch 2/3, Train Loss: 0.1138, Test Loss: 0.0857, Test Acc: 0.9739\n",
            "Epoch 3/3, Train Loss: 0.0831, Test Loss: 0.0785, Test Acc: 0.9770\n",
            "Epoch 1/3, Train Loss: 0.3722, Test Loss: 0.2972, Test Acc: 0.9173\n",
            "Epoch 2/3, Train Loss: 0.3075, Test Loss: 0.2510, Test Acc: 0.9363\n",
            "Epoch 3/3, Train Loss: 0.2834, Test Loss: 0.2267, Test Acc: 0.9443\n",
            "Epoch 1/3, Train Loss: 0.4039, Test Loss: 0.2711, Test Acc: 0.9305\n",
            "Epoch 2/3, Train Loss: 0.3574, Test Loss: 0.2660, Test Acc: 0.9289\n",
            "Epoch 3/3, Train Loss: 0.3397, Test Loss: 0.2893, Test Acc: 0.9365\n",
            "Epoch 1/3, Train Loss: 0.3709, Test Loss: 0.2437, Test Acc: 0.9336\n",
            "Epoch 2/3, Train Loss: 0.3138, Test Loss: 0.2439, Test Acc: 0.9414\n",
            "Epoch 3/3, Train Loss: 0.2952, Test Loss: 0.2075, Test Acc: 0.9462\n",
            "Epoch 1/3, Train Loss: 0.4077, Test Loss: 0.3196, Test Acc: 0.9235\n",
            "Epoch 2/3, Train Loss: 0.3584, Test Loss: 0.2453, Test Acc: 0.9406\n",
            "Epoch 3/3, Train Loss: 0.3407, Test Loss: 0.3193, Test Acc: 0.9317\n",
            "Epoch 1/3, Train Loss: 0.3743, Test Loss: 0.2721, Test Acc: 0.9294\n",
            "Epoch 2/3, Train Loss: 0.3012, Test Loss: 0.2013, Test Acc: 0.9437\n",
            "Epoch 3/3, Train Loss: 0.2786, Test Loss: 0.1985, Test Acc: 0.9485\n",
            "Epoch 1/3, Train Loss: 0.4116, Test Loss: 0.2476, Test Acc: 0.9308\n",
            "Epoch 2/3, Train Loss: 0.3562, Test Loss: 0.2508, Test Acc: 0.9325\n",
            "Epoch 3/3, Train Loss: 0.3360, Test Loss: 0.2789, Test Acc: 0.9402\n",
            "Epoch 1/3, Train Loss: 0.4012, Test Loss: 0.2812, Test Acc: 0.9215\n",
            "Epoch 2/3, Train Loss: 0.3418, Test Loss: 0.2429, Test Acc: 0.9405\n",
            "Epoch 3/3, Train Loss: 0.3388, Test Loss: 0.2528, Test Acc: 0.9396\n",
            "Epoch 1/3, Train Loss: 0.4307, Test Loss: 0.2732, Test Acc: 0.9209\n",
            "Epoch 2/3, Train Loss: 0.3763, Test Loss: 0.2912, Test Acc: 0.9234\n",
            "Epoch 3/3, Train Loss: 0.3552, Test Loss: 0.2683, Test Acc: 0.9254\n",
            "Epoch 1/3, Train Loss: 0.4226, Test Loss: 0.2914, Test Acc: 0.9228\n",
            "Epoch 2/3, Train Loss: 0.3721, Test Loss: 0.3360, Test Acc: 0.9249\n",
            "Epoch 3/3, Train Loss: 0.3523, Test Loss: 0.3308, Test Acc: 0.9176\n",
            "Epoch 1/3, Train Loss: 0.3532, Test Loss: 0.2165, Test Acc: 0.9386\n",
            "Epoch 2/3, Train Loss: 0.2971, Test Loss: 0.2728, Test Acc: 0.9262\n",
            "Epoch 3/3, Train Loss: 0.2791, Test Loss: 0.2361, Test Acc: 0.9433\n",
            "Epoch 1/3, Train Loss: 0.4070, Test Loss: 0.2834, Test Acc: 0.9202\n",
            "Epoch 2/3, Train Loss: 0.3396, Test Loss: 0.2562, Test Acc: 0.9370\n",
            "Epoch 3/3, Train Loss: 0.3259, Test Loss: 0.2602, Test Acc: 0.9379\n",
            "Epoch 1/3, Train Loss: 0.3989, Test Loss: 0.2685, Test Acc: 0.9241\n",
            "Epoch 2/3, Train Loss: 0.3479, Test Loss: 0.2728, Test Acc: 0.9269\n",
            "Epoch 3/3, Train Loss: 0.3338, Test Loss: 0.2797, Test Acc: 0.9338\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n PSO\")\n",
        "best_params, best_score, results = PSO_hyperparam(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb7dv5CPCkCH"
      },
      "outputs": [],
      "source": [
        "print(\"\\n RL PSO\")\n",
        "best_params, best_score, results = RL_PSO_hyperparam(train_mnist, param_grid_mnist)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX9iMZ79CtXA"
      },
      "outputs": [],
      "source": [
        "param_grid_cifar10 = {\n",
        "    'filters1': [32, 64, 128],\n",
        "    'filters2': [64, 128, 256],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3],\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'epochs': 3,  # Fixed for faster execution\n",
        "    'optimizer_type': ['adam', 'sgd']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5af3xCBCl7e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nExample 2: CIFAR-10\")\n",
        "print(\"\\nRandom Search\")\n",
        "best_params, best_score, results = random_search(train_cifar10, param_grid_cifar10, n_trials=5)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kgb9zn0C0um"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Grid Search\")\n",
        "best_params, best_score, results = grid_search(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjDNcseiYRcl"
      },
      "outputs": [],
      "source": [
        "print(\"\\nBayesian Optimization\")\n",
        "from skopt.space import Real, Integer\n",
        "param_space_cifar10 = [\n",
        "    Integer(32, 128, name='filters1'),\n",
        "    Integer(64, 256, name='filters2'),\n",
        "    Real(0.1, 0.3, name='dropout_rate'),\n",
        "    Real(1e-4, 1e-2, prior='log-uniform', name='learning_rate'),\n",
        "    Integer(32, 128, name='batch_size'),\n",
        "    Integer(3, 10, name='epochs')\n",
        "]\n",
        "\n",
        "best_params, best_score = bayesian_optimization(\n",
        "    train_cifar10,\n",
        "    param_space_cifar10,\n",
        "    n_calls=5\n",
        ")\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1Pk40XAC4gQ"
      },
      "outputs": [],
      "source": [
        "print(\"\\n PSO\")\n",
        "best_params, best_score, results = PSO(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiWqaejRC7RG"
      },
      "outputs": [],
      "source": [
        "print(\"\\n RL PSO\")\n",
        "best_params, best_score, results = RL_PSO(train_cifar10, param_grid_cifar10)\n",
        "print(f\"Best accuracy: {best_score:.4f}\")\n",
        "print(f\"Best parameters: {best_params}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "na",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
