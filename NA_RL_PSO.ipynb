{"cells":[{"cell_type":"code","source":["import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm"],"metadata":{"id":"o1EJufEb42Mj","executionInfo":{"status":"ok","timestamp":1743134560586,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sohan Thummala","userId":"14370283689667505630"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Define objective function to minimize (Rastrigin function)\n","def rastrigin(x):\n","    A = 10\n","    return A * len(x) + sum([(xi ** 2 - A * np.cos(2 * np.pi * xi)) for xi in x])"],"metadata":{"id":"JRtB1CSO45AQ","executionInfo":{"status":"ok","timestamp":1743134352520,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sohan Thummala","userId":"14370283689667505630"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Particle Swarm Optimization implementation (baseline)\n","class Particle:\n","    def __init__(self, dim, bounds, objective=rastrigin):\n","        self.position = np.array([random.uniform(bounds[0], bounds[1]) for _ in range(dim)])\n","        self.velocity = np.zeros(dim)\n","        self.best_position = np.copy(self.position)\n","        self.best_value = objective(self.position)\n","\n","class PSO:\n","    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin):\n","        self.dim = dim\n","        self.bounds = bounds\n","        self.num_particles = num_particles\n","        self.num_iter = num_iter\n","        self.objective = objective\n","        self.swarm = [Particle(dim, bounds, objective) for _ in range(num_particles)]\n","        self.global_best_position = np.copy(self.swarm[0].position)\n","        self.global_best_value = objective(self.global_best_position)\n","\n","    def optimize(self):\n","        w = 0.5\n","        c1, c2 = 1.5, 1.5\n","        for _ in range(self.num_iter):\n","            for particle in self.swarm:\n","                r1, r2 = random.random(), random.random()\n","                particle.velocity = (w * particle.velocity +\n","                                     c1 * r1 * (particle.best_position - particle.position) +\n","                                     c2 * r2 * (self.global_best_position - particle.position))\n","                particle.position = np.clip(particle.position + particle.velocity, self.bounds[0], self.bounds[1])\n","                value = self.objective(particle.position)\n","                if value < particle.best_value:\n","                    particle.best_position, particle.best_value = particle.position, value\n","                if value < self.global_best_value:\n","                    self.global_best_position, self.global_best_value = particle.position, value\n","        return self.global_best_position, self.global_best_value"],"metadata":{"id":"Rn_BWhFM5tDw","executionInfo":{"status":"ok","timestamp":1743134561684,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sohan Thummala","userId":"14370283689667505630"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Policy Network for the RL component\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_size=16):\n","        super(PolicyNetwork, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.action_mean = nn.Linear(hidden_size, action_dim)\n","        # Learnable log_std parameter for the Gaussian distribution\n","        self.log_std = nn.Parameter(torch.zeros(action_dim))\n","\n","    def forward(self, state):\n","        x = torch.relu(self.fc1(state))\n","        x = torch.relu(self.fc2(x))\n","        mean = self.action_mean(x)\n","        std = torch.exp(self.log_std)\n","        return mean, std\n","\n","    def select_action(self, state):\n","        mean, std = self.forward(state)\n","        dist = torch.distributions.Normal(mean, std)\n","        action = dist.sample()\n","        log_prob = dist.log_prob(action).sum()  # Sum log probabilities across dimensions\n","        # Clamp actions to a plausible range for c1 and c2 (e.g., [1.0, 2.0])\n","        action = torch.clamp(action, 1.0, 2.0)\n","        return action, log_prob\n","\n","# RL-enhanced PSO using the policy network to select acceleration coefficients c1 and c2\n","class RLPSO(PSO):\n","    def __init__(self, dim, bounds, num_particles=30, num_iter=100, objective=rastrigin, lr=1e-2):\n","        super().__init__(dim, bounds, num_particles, num_iter, objective)\n","        # Define a simple state: [iteration_ratio, global_best_value]\n","        self.state_dim = 2\n","        self.action_dim = 2  # one for c1 and one for c2\n","        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n","        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n","        self.objective = objective\n","\n","    def get_state(self, iteration):\n","        # Normalize iteration to [0, 1]\n","        iter_norm = iteration / self.num_iter\n","        # Use the current global best value (optionally, further normalization can be applied)\n","        state = np.array([iter_norm, self.global_best_value], dtype=np.float32)\n","        return torch.tensor(state)\n","\n","    def optimize(self):\n","        w = 0.5\n","        for it in tqdm(range(1, self.num_iter + 1), desc=\"Training progress\"):\n","            state = self.get_state(it)\n","            action, log_prob = self.policy_net.select_action(state)\n","            c1, c2 = action.detach().numpy()  # use the selected coefficients for this iteration\n","\n","            # Store the old global best for reward computation\n","            old_global_best = self.global_best_value\n","\n","            for particle in self.swarm:\n","                r1, r2 = random.random(), random.random()\n","                particle.velocity = (w * particle.velocity +\n","                                     c1 * r1 * (particle.best_position - particle.position) +\n","                                     c2 * r2 * (self.global_best_position - particle.position))\n","                particle.position = np.clip(particle.position + particle.velocity, self.bounds[0], self.bounds[1])\n","                value = self.objective(particle.position)\n","                if value < particle.best_value:\n","                    particle.best_position, particle.best_value = particle.position, value\n","                if value < self.global_best_value:\n","                    self.global_best_position, self.global_best_value = particle.position, value\n","\n","            # Reward: improvement in global best value\n","            reward = old_global_best - self.global_best_value\n","            reward_tensor = torch.tensor(reward, dtype=torch.float32)\n","\n","            # Update policy network using the REINFORCE rule: maximize reward\n","            loss = -log_prob * reward_tensor\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","\n","        return self.global_best_position, self.global_best_value\n"],"metadata":{"id":"SKfEtN_r4y_r","executionInfo":{"status":"ok","timestamp":1743134563359,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sohan Thummala","userId":"14370283689667505630"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Testing both the baseline PSO and the RL-enhanced PSO\n","dim = 20\n","bounds = (-5.12, 5.12)\n","num_particles = 30\n","num_iter = 100\n","\n","print(\"Running baseline PSO...\")\n","pso_solver = PSO(dim, bounds, num_particles, num_iter)\n","best_position_pso, best_value_pso = pso_solver.optimize()\n","print(\"\\nBaseline PSO solution:\")\n","print(\"Best position:\", best_position_pso)\n","print(\"Best value:\", best_value_pso)\n","\n","print(\"\\nRunning RL-enhanced PSO (RLPSO)...\")\n","rl_pso_solver = RLPSO(dim, bounds, num_particles, num_iter)\n","best_position_rl, best_value_rl = rl_pso_solver.optimize()\n","print(\"\\nRLPSO solution:\")\n","print(\"Best position:\", best_position_rl)\n","print(\"Best value:\", best_value_rl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hcksn8R5xh8","executionInfo":{"status":"ok","timestamp":1743134688725,"user_tz":-330,"elapsed":692,"user":{"displayName":"Sohan Thummala","userId":"14370283689667505630"}},"outputId":"384792bc-8234-47f3-ccf6-7b821364e17c"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Running baseline PSO...\n","\n","Baseline PSO solution:\n","Best position: [ 1.0235498   0.95176271 -2.15361613  2.96116039  4.02970897  0.02861689\n"," -2.96370063  0.09227535  0.95874     1.0464803   1.03747529 -0.05790331\n","  3.00524706  1.02219839  1.87580394  0.01068383  1.99013958  0.02099836\n"," -0.09674651 -0.93212351]\n","Best value: 76.81647433027369\n","\n","Running RL-enhanced PSO (RLPSO)...\n"]},{"output_type":"stream","name":"stderr","text":["Training progress: 100%|██████████| 100/100 [00:00<00:00, 224.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","RLPSO solution:\n","Best position: [ 0.05631228 -0.1991127  -0.96856322 -2.92018528  2.98497274 -0.06295822\n","  0.98502784  0.91351776 -1.14774585 -0.97165242  1.05866453 -0.02145037\n"," -1.14531822 -0.84982772  0.82602287  0.95595066 -0.06202925  0.1184671\n"," -3.04233934 -0.02878297]\n","Best value: 70.35736406913466\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}